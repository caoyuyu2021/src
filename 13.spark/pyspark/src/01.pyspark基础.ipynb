{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97300d23",
   "metadata": {},
   "source": [
    "# 引言"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5345af1",
   "metadata": {},
   "source": [
    "关于PySpark，我们知道它是Python调用Spark的接口，我们可以通过调用Python API的方式来编写Spark程序，它支持了大多数的Spark功能，比如SparkDataFrame、Spark SQL、Streaming、MLlib等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f633efc",
   "metadata": {},
   "source": [
    "![spark](../images/spark.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710cd0b",
   "metadata": {},
   "source": [
    "[Windows和Jupyter notebook配置pyspark](https://sparkbyexamples.com/pyspark/install-pyspark-in-anaconda-jupyter-notebook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7260a6",
   "metadata": {},
   "source": [
    "在ipython中引入pyspark  \n",
    "`import findspark`  \n",
    "`findspark.init()`  \n",
    "然后就可以自由调用pyspark的API了."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ef6a0",
   "metadata": {},
   "source": [
    "# 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7dfeb",
   "metadata": {},
   "source": [
    "## 什么是RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44677071",
   "metadata": {},
   "source": [
    "RDD的全称是 Resilient Distributed Datasets（弹性分布式数据集），这是Spark的一种数据抽象集合，它可以被执行在分布式的集群上进行各种操作，而且有较强的容错机制。RDD可以被分为若干个分区，每一个分区就是一个数据集片段，从而可以支持分布式计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e54f3",
   "metadata": {},
   "source": [
    "## RDD运行时相关的关键名词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291197f5",
   "metadata": {},
   "source": [
    "- Client：指的是客户端进程，主要负责提交job到Master；\n",
    "\n",
    "- Job：Job来自于我们编写的程序，Application包含一个或者多个job，job包含各种RDD操作；\n",
    "\n",
    "- Master：指的是Standalone模式中的主控节点，负责接收来自Client的job，并管理着worker，可以给worker分配任务和资源（主要是driver和executor资源）；\n",
    "\n",
    "- Worker：指的是Standalone模式中的slave节点，负责管理本节点的资源，同时受Master管理，需要定期给Master回报heartbeat（心跳），启动Driver和Executor；\n",
    "\n",
    "- Driver：指的是 job（作业）的主进程，一般每个Spark作业都会有一个Driver进程，负责整个作业的运行，包括了job的解析、Stage的生成、调度Task到Executor上去执行；\n",
    "\n",
    "- Stage：中文名 阶段，是job的基本调度单位，因为每个job会分成若干组Task，每组任务就被称为 Stage；\n",
    "\n",
    "- Task：任务，指的是直接运行在executor上的东西，是executor上的一个线程；\n",
    "\n",
    "- Executor：指的是 执行器，顾名思义就是真正执行任务的地方了，一个集群可以被配置若干个Executor，每个Executor接收来自Driver的Task，并执行它（可同时执行多个Task）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db1ea1",
   "metadata": {},
   "source": [
    "## 什么是DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc5f0d",
   "metadata": {},
   "source": [
    "全称是 Directed Acyclic Graph，中文名是有向无环图。Spark就是借用了DAG对RDD之间的关系进行了建模，用来描述RDD之间的因果依赖关系。因为在一个Spark作业调度中，多个作业任务之间也是相互依赖的，有些任务需要在一些任务执行完成了才可以执行的。在Spark调度中就是有DAGscheduler，它负责将job分成若干组Task组成的Stage。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e5b91",
   "metadata": {},
   "source": [
    "## Spark的部署模式有哪些"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e75b41",
   "metadata": {},
   "source": [
    "主要有local模式、Standalone模式、Mesos模式、YARN模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d5505",
   "metadata": {},
   "source": [
    "- Standalone： 独立模式，Spark 原生的简单集群管理器， 自带完整的服务， 可单独部署到一个集群中，无需依赖任何其他资源管理系统， 使用 Standalone 可以很方便地搭建一个集群，一般在公司内部没有搭建其他资源管理框架的时候才会使用。\n",
    "- Mesos：一个强大的分布式资源管理框架，它允许多种不同的框架部署在其上，包括 yarn，由于mesos这种方式目前应用的比较少，这里没有记录mesos的部署方式。\n",
    "- YARN： 统一的资源管理机制， 在上面可以运行多套计算框架， 如map reduce、storm 等， 根据 driver 在集群中的位置不同，分为 yarn client 和 yarn cluster。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b38d6",
   "metadata": {},
   "source": [
    "Spark 的运行模式取决于传递给 SparkContext 的 MASTER 环境变量的值， 个别模式还需要辅助的程序接口来配合使用，目前支持的 Master 字符串及 URL 包括："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61a826",
   "metadata": {},
   "source": [
    "|MasterURL\t|Meaning|\n",
    "|:--|:--|\n",
    "|local\t|在本地运行，只有一个工作进程，无并行计算能力\n",
    "|local[K]|\t在本地运行，有 K 个工作进程，通常设置 K 为机器的CPU 核心数量\n",
    "|local[*]|\t在本地运行，工作进程数量等于机器的 CPU 核心数量。\n",
    "|spark://HOST:PORT\t|以 Standalone 模式运行，这是 Spark 自身提供的集群运行模式，默认端口号: 7077\n",
    "|mesos://HOST:PORT\t|在 Mesos 集群上运行，Driver 进程和 Worker 进程运行在 Mesos 集群上，部署模式必须使用固定值:--deploy-mode cluster\n",
    "|yarn-client|\t在 Yarn 集群上运行，Driver 进程在本地， Work 进程在 Yarn 集群上， 部署模式必须使用固定值:--deploy-modeclient。Yarn 集群地址必须在HADOOP_CONF_DIRorYARN_CONF_DIR 变量里定义。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1118e3",
   "metadata": {},
   "source": [
    "## Shuffle操作是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f131a69",
   "metadata": {},
   "source": [
    "Shuffle指的是数据从Map端到Reduce端的数据传输过程，Shuffle性能的高低直接会影响程序的性能。因为Reduce task需要跨节点去拉在分布在不同节点上的Map task计算结果，这一个过程是需要有磁盘IO消耗以及数据网络传输的消耗的，所以需要根据实际数据情况进行适当调整。另外，Shuffle可以分为两部分，分别是Map阶段的数据准备与Reduce阶段的数据拷贝处理，在Map端我们叫Shuffle Write，在Reduce端我们叫Shuffle Read。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1730aa",
   "metadata": {},
   "source": [
    "## 什么是惰性执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d6d9f",
   "metadata": {},
   "source": [
    "这是RDD的一个特性，在RDD中的算子可以分为Transform算子和Action算子，其中Transform算子的操作都不会真正执行，只会记录一下依赖关系，直到遇见了Action算子，在这之前的所有Transform操作才会被触发计算，这就是所谓的惰性执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0d2cf",
   "metadata": {},
   "source": [
    "- Transform算子:map、flatMap、filter、distinct、reduceByKey、mapPartitions、sortBy\n",
    "- Action算子:collect、collectAsMap、reduce、countByKey、take、first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f3c6c",
   "metadata": {},
   "source": [
    "# 常用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c1f6e",
   "metadata": {},
   "source": [
    "[官方API文档](https://spark.apache.org/docs/latest/api/python/reference/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab77e779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:58:26.655543Z",
     "start_time": "2023-10-20T07:58:26.599367Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=test, master=local[4]) created by __init__ at C:\\Users\\IKAS\\AppData\\Local\\Temp\\ipykernel_20392\\526825949.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[0;32m      3\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[4]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark version:\u001b[39m\u001b[38;5;124m\"\u001b[39m,pyspark\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      7\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pyspark\\context.py:445\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    450\u001b[0m             currentAppName,\n\u001b[0;32m    451\u001b[0m             currentMaster,\n\u001b[0;32m    452\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m    453\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[0;32m    454\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[0;32m    455\u001b[0m         )\n\u001b[0;32m    456\u001b[0m     )\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=test, master=local[4]) created by __init__ at C:\\Users\\IKAS\\AppData\\Local\\Temp\\ipykernel_20392\\526825949.py:4 "
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"test\").setMaster(\"local[4]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "print(\"spark version:\",pyspark.__version__)\n",
    "rdd = sc.parallelize([\"hello\",\"spark\"])\n",
    "print(rdd.reduce(lambda x,y:x+' '+y))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aefa8b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:58:42.594720Z",
     "start_time": "2023-10-20T07:58:42.494981Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "197683b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:58:19.057679Z",
     "start_time": "2023-10-20T07:58:19.054207Z"
    }
   },
   "outputs": [],
   "source": [
    "#在jupyter中使用pyspark\n",
    "import findspark\n",
    "#findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e037e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:58:20.034085Z",
     "start_time": "2023-10-20T07:58:20.030287Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_home = \"D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\pyspark\"\n",
    "python_path = \"D:\\\\Anaconda\\\\python\"\n",
    "findspark.init(spark_home,python_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ca18df1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:58:45.128372Z",
     "start_time": "2023-10-20T07:58:44.835136Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 使用 parallelize方法直接实例化一个RDD\u001b[39;00m\n\u001b[0;32m      9\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m11\u001b[39m),\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# 这里的 4 指的是分区数量\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m rdd\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pyspark\\rdd.py:2836\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2833\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2835\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2836\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m, takeUpToNumLeft, p)\n\u001b[0;32m   2838\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2839\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pyspark\\context.py:2319\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2317\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2318\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2319\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd, partitions)\n\u001b[0;32m   2320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pyspark\\rdd.py:5441\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5439\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 5441\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m _wrap_function(\n\u001b[0;32m   5442\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd_deserializer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer, profiler\n\u001b[0;32m   5443\u001b[0m )\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[0;32m   5447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[0;32m   5448\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pyspark\\rdd.py:5243\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   5241\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m   5242\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 5243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[0;32m   5244\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[0;32m   5245\u001b[0m     env,\n\u001b[0;32m   5246\u001b[0m     includes,\n\u001b[0;32m   5247\u001b[0m     sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[0;32m   5248\u001b[0m     sc\u001b[38;5;241m.\u001b[39mpythonVer,\n\u001b[0;32m   5249\u001b[0m     broadcast_vars,\n\u001b[0;32m   5250\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[0;32m   5251\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"test_SamShare\").setMaster(\"local[4]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# 使用 parallelize方法直接实例化一个RDD\n",
    "rdd = sc.parallelize(range(1,11),4) # 这里的 4 指的是分区数量\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fff9961",
   "metadata": {},
   "source": [
    "## Transform算子解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88074c2",
   "metadata": {},
   "source": [
    "以下的操作由于是Transform操作，因为我们需要在最后加上一个collect算子用来触发计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf3d5c",
   "metadata": {},
   "source": [
    "### map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6649e2f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T05:39:58.556006Z",
     "start_time": "2023-10-20T05:39:50.054511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "扩大2倍： [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
     ]
    }
   ],
   "source": [
    "# 1. map: 和python差不多，map转换就是对每一个元素进行一个映射\n",
    "rdd = sc.parallelize(range(1, 11), 4)\n",
    "rdd_map = rdd.map(lambda x: x*2)\n",
    "print(\"原始数据：\", rdd.collect())\n",
    "print(\"扩大2倍：\", rdd_map.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014dfe31",
   "metadata": {},
   "source": [
    "### flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4bbd48f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T02:56:16.036200Z",
     "start_time": "2023-09-13T02:56:08.860783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据： ['hello SamShare', 'hello PySpark']\n",
      "直接split之后的map结果： [['hello', 'SamShare'], ['hello', 'PySpark']]\n",
      "直接split之后的flatMap结果： ['hello', 'SamShare', 'hello', 'PySpark']\n"
     ]
    }
   ],
   "source": [
    "# 2. flatMap: 这个相比于map多一个flat（压平）操作，顾名思义就是要把高维的数组变成一维\n",
    "rdd2 = sc.parallelize([\"hello SamShare\", \"hello PySpark\"])\n",
    "print(\"原始数据：\", rdd2.collect())\n",
    "print(\"直接split之后的map结果：\", rdd2.map(lambda x: x.split(\" \")).collect())\n",
    "print(\"直接split之后的flatMap结果：\", rdd2.flatMap(lambda x: x.split(\" \")).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffdd1fb",
   "metadata": {},
   "source": [
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70310e00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T02:58:15.782010Z",
     "start_time": "2023-09-13T02:58:08.637457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "过滤奇数： [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# 3. filter: 过滤数据\n",
    "rdd = sc.parallelize(range(1, 11), 4)\n",
    "print(\"原始数据：\", rdd.collect())\n",
    "print(\"过滤奇数：\", rdd.filter(lambda x: x % 2 == 0).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ced3d",
   "metadata": {},
   "source": [
    "### distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ffe39ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T02:59:07.835380Z",
     "start_time": "2023-09-13T02:59:00.458420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据： [2, 2, 4, 8, 8, 8, 8, 16, 32, 32]\n",
      "去重数据： [4, 8, 16, 32, 2]\n"
     ]
    }
   ],
   "source": [
    "# 4. distinct: 去重元素\n",
    "rdd = sc.parallelize([2, 2, 4, 8, 8, 8, 8, 16, 32, 32])\n",
    "print(\"原始数据：\", rdd.collect())\n",
    "print(\"去重数据：\", rdd.distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bac278",
   "metadata": {},
   "source": [
    "### reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d00ae275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:10:33.082438Z",
     "start_time": "2023-09-13T03:10:25.831807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据： [('a', 1), ('b', 1), ('a', 1)]\n",
      "原始数据： [('b', 1), ('a', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 5. reduceByKey: 根据key来映射数据\n",
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "print(\"原始数据：\", rdd.collect())\n",
    "print(\"原始数据：\", rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbc434",
   "metadata": {},
   "source": [
    "### mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbb55218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:11:19.351916Z",
     "start_time": "2023-09-13T03:11:17.475774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[3, 7]\n"
     ]
    }
   ],
   "source": [
    "# 6. mapPartitions: 根据分区内的数据进行映射操作\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "def f(iterator):\n",
    "    yield sum(iterator)\n",
    "print(rdd.collect())\n",
    "print(rdd.mapPartitions(f).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0031ded",
   "metadata": {},
   "source": [
    "### sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c29c506f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:13:11.301975Z",
     "start_time": "2023-09-13T03:12:43.079472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n"
     ]
    }
   ],
   "source": [
    "# 7. sortBy: 根据规则进行排序\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "print(sc.parallelize(tmp).sortBy(lambda x: x[0]).collect())\n",
    "print(sc.parallelize(tmp).sortBy(lambda x: x[1]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f8a01",
   "metadata": {},
   "source": [
    "### subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac08cda6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:14:00.022706Z",
     "start_time": "2023-09-13T03:13:39.118159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 4), ('b', 5)]\n"
     ]
    }
   ],
   "source": [
    "# 8. subtract: 数据集相减, Return each value in self that is not contained in other.\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "print(sorted(x.subtract(y).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f21e53",
   "metadata": {},
   "source": [
    "### union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b2c7985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:14:28.440914Z",
     "start_time": "2023-09-13T03:14:28.380992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 1, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# 9. union: 合并两个RDD\n",
    "rdd = sc.parallelize([1, 1, 2, 3])\n",
    "print(rdd.union(rdd).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062935e7",
   "metadata": {},
   "source": [
    "### intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd4b0ef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:15:15.754596Z",
     "start_time": "2023-09-13T03:14:52.612327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# 10. intersection: 取两个RDD的交集，同时有去重的功效\n",
    "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5, 2, 3])\n",
    "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "print(rdd1.intersection(rdd2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb21b21",
   "metadata": {},
   "source": [
    "### cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36593f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:15:24.705867Z",
     "start_time": "2023-09-13T03:15:24.615863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 2), (2, 1), (2, 2)]\n"
     ]
    }
   ],
   "source": [
    "# 11. cartesian: 生成笛卡尔积\n",
    "rdd = sc.parallelize([1, 2])\n",
    "print(sorted(rdd.cartesian(rdd).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd30596",
   "metadata": {},
   "source": [
    "### zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09ec34a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:16:04.719015Z",
     "start_time": "2023-09-13T03:15:49.319600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1000, 1001, 1002, 1003, 1004]\n",
      "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n"
     ]
    }
   ],
   "source": [
    "# 12. zip: 拉链合并，需要两个RDD具有相同的长度以及分区数量\n",
    "x = sc.parallelize(range(0, 5))\n",
    "y = sc.parallelize(range(1000, 1005))\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(x.zip(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26ce02",
   "metadata": {},
   "source": [
    "### zipWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4132a6a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:16:35.098129Z",
     "start_time": "2023-09-13T03:16:27.383700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LiLei', 0), ('Hanmeimei', 1), ('Lily', 2), ('Lucy', 3), ('Ann', 4), ('Dachui', 5), ('RuHua', 6)]\n"
     ]
    }
   ],
   "source": [
    "# 13. zipWithIndex: 将RDD和一个从0开始的递增序列按照拉链方式连接。\n",
    "rdd_name = sc.parallelize([\"LiLei\", \"Hanmeimei\", \"Lily\", \"Lucy\", \"Ann\", \"Dachui\", \"RuHua\"])\n",
    "rdd_index = rdd_name.zipWithIndex()\n",
    "print(rdd_index.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff7cd7",
   "metadata": {},
   "source": [
    "### groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "595448f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:17:29.623902Z",
     "start_time": "2023-09-13T03:17:14.254652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 1), ('a', 1)]\n",
      "[('a', 2), ('b', 1)]\n",
      "[('a', [1, 1]), ('b', [1])]\n"
     ]
    }
   ],
   "source": [
    "# 14. groupByKey: 按照key来聚合数据\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "print(rdd.collect())\n",
    "print(sorted(rdd.groupByKey().mapValues(len).collect()))\n",
    "print(sorted(rdd.groupByKey().mapValues(list).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e8458",
   "metadata": {},
   "source": [
    "### sortByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bef24d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:18:04.869959Z",
     "start_time": "2023-09-13T03:18:03.787338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n"
     ]
    }
   ],
   "source": [
    "# 15. sortByKey:\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "print(sc.parallelize(tmp).sortByKey(True, 1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e03854",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "626426c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:19:00.634714Z",
     "start_time": "2023-09-13T03:18:37.458296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, 2)), ('a', (1, 3))]\n"
     ]
    }
   ],
   "source": [
    "# 16. join:\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "print(sorted(x.join(y).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2193c4",
   "metadata": {},
   "source": [
    "### leftOuterJoin/rightOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92a108ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:20:07.813279Z",
     "start_time": "2023-09-13T03:19:42.725415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, 2)), ('b', (4, None))]\n"
     ]
    }
   ],
   "source": [
    "# 17. leftOuterJoin/rightOuterJoin\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "print(sorted(x.leftOuterJoin(y).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0abd79",
   "metadata": {},
   "source": [
    "## Action算子解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933bf02b",
   "metadata": {},
   "source": [
    "### collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef912bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:20:48.924279Z",
     "start_time": "2023-09-13T03:20:44.794416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 1. collect: 指的是把数据都汇集到driver端，便于后续的操作\n",
    "rdd = sc.parallelize(range(0, 5))\n",
    "rdd_collect = rdd.collect()\n",
    "print(rdd_collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c79df8",
   "metadata": {},
   "source": [
    "### first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86ab5821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:21:10.955225Z",
     "start_time": "2023-09-13T03:21:06.334220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. first: 取第一个元素\n",
    "sc.parallelize([2, 3, 4]).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7eb9a2",
   "metadata": {},
   "source": [
    "### collectAsMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41912ed7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:21:37.920711Z",
     "start_time": "2023-09-13T03:21:37.876067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 3: 4}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. collectAsMap: 转换为dict，使用这个要注意了，不要对大数据用，不然全部载入到driver端会爆内存\n",
    "m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689da831",
   "metadata": {},
   "source": [
    "### reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96e5ab27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:22:27.401508Z",
     "start_time": "2023-09-13T03:22:22.330198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# 4. reduce: 逐步对两个元素进行操作\n",
    "rdd = sc.parallelize(range(10),5)\n",
    "print(rdd.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd93fe",
   "metadata": {},
   "source": [
    "### countByKey/countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b544fe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:23:32.320810Z",
     "start_time": "2023-09-13T03:23:24.258094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 1)]\n",
      "[(('a', 1), 2), (('b', 1), 1)]\n"
     ]
    }
   ],
   "source": [
    "# 5. countByKey/countByValue:\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "print(sorted(rdd.countByKey().items()))\n",
    "print(sorted(rdd.countByValue().items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e142368",
   "metadata": {},
   "source": [
    "### take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "990d96e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:24:07.424232Z",
     "start_time": "2023-09-13T03:24:03.230654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 1), ('a', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 6. take: 相当于取几个数据到driver端\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "print(rdd.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ff2878",
   "metadata": {},
   "source": [
    "### saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce694673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. saveAsTextFile: 保存rdd成text文件到本地\n",
    "text_file = \"./data/rdd.txt\"\n",
    "rdd = sc.parallelize(range(5))\n",
    "rdd.saveAsTextFile(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cb74e",
   "metadata": {},
   "source": [
    "### takeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da712343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. takeSample: 随机取数\n",
    "rdd = sc.textFile(\"./test/data/hello_samshare.txt\", 4)  # 这里的 4 指的是分区数量\n",
    "rdd_sample = rdd.takeSample(True, 2, 0)  # withReplacement 参数1：代表是否是有放回抽样\n",
    "rdd_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed7e42",
   "metadata": {},
   "source": [
    "### foreach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87b94ceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T03:27:06.528177Z",
     "start_time": "2023-09-13T03:27:01.986490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# 9. foreach: 对每一个元素执行某种操作，不生成新的RDD\n",
    "rdd = sc.parallelize(range(10), 5)\n",
    "accum = sc.accumulator(0)\n",
    "rdd.foreach(lambda x: accum.add(x))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d102e",
   "metadata": {},
   "source": [
    "# Spark SQL使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3bce1",
   "metadata": {},
   "source": [
    "这个模块是Spark中用来处理结构化数据的，提供一个叫SparkDataFrame的东西并且自动解析为分布式SQL查询数据。我们之前用过Python的Pandas库，也大致了解了DataFrame，这个其实和它没有太大的区别，只是调用的API可能有些不同罢了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c76396",
   "metadata": {},
   "source": [
    "# Spark调优思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef8ea1",
   "metadata": {},
   "source": [
    "## 开发习惯调优"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63afa6ff",
   "metadata": {},
   "source": [
    "### 尽可能复用同一个RDD，避免重复创建，并且适当持久化数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9c326",
   "metadata": {},
   "source": [
    "这种开发习惯是需要我们对于即将要开发的应用逻辑有比较深刻的思考，并且可以通过code review来发现的，讲白了就是要记得我们创建过啥数据集，可以复用的尽量广播（broadcast）下，能很好提升性能。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b200ff63",
   "metadata": {},
   "source": [
    "# 最低级写法，相同数据集重复创建。\n",
    "rdd1 = sc.textFile(\"./test/data/hello_samshare.txt\", 4) # 这里的 4 指的是分区数量\n",
    "rdd2 = sc.textFile(\"./test/data/hello_samshare.txt\", 4) # 这里的 4 指的是分区数量\n",
    "print(rdd1.take(10))\n",
    "print(rdd2.map(lambda x:x[0:1]).take(10))\n",
    "\n",
    "# 稍微进阶一些，复用相同数据集，但因中间结果没有缓存，数据会重复计算\n",
    "rdd1 = sc.textFile(\"./test/data/hello_samshare.txt\", 4) # 这里的 4 指的是分区数量\n",
    "print(rdd1.take(10))\n",
    "print(rdd1.map(lambda x:x[0:1]).take(10))\n",
    "\n",
    "# 相对比较高效，使用缓存来持久化数据\n",
    "rdd = sc.parallelize(range(1, 11), 4).cache()  # 或者persist()\n",
    "rdd_map = rdd.map(lambda x: x*2)\n",
    "rdd_reduce = rdd.reduce(lambda x, y: x+y)\n",
    "print(rdd_map.take(10))\n",
    "print(rdd_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1b940",
   "metadata": {},
   "source": [
    "### 尽量避免使用低性能算子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b9582",
   "metadata": {},
   "source": [
    "shuffle类算子算是低性能算子的一种代表，所谓的shuffle类算子，指的是会产生shuffle过程的操作，就是需要把各个节点上的相同key写入到本地磁盘文件中，然后其他的节点通过网络传输拉取自己需要的key，把相同key拉到同一个节点上进行聚合计算，这种操作必然就是有大量的数据网络传输与磁盘读写操作，性能往往不是很好的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71611b6",
   "metadata": {},
   "source": [
    "### 尽量使用高性能算子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279952f3",
   "metadata": {},
   "source": [
    "### 广播大变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12373e",
   "metadata": {},
   "source": [
    "如果我们有一个数据集很大，并且在后续的算子执行中会被反复调用，那么就建议直接把它广播（broadcast）一下。当变量被广播后，会保证每个executor的内存中只会保留一份副本，同个executor内的task都可以共享这个副本数据。如果没有广播，常规过程就是把大变量进行网络传输到每一个相关task中去，这样子做，一来频繁的网络数据传输，效率极其低下；二来executor下的task不断存储同一份大数据，很有可能就造成了内存溢出或者频繁GC，效率也是极其低下的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a0b30304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T06:17:42.496960Z",
     "start_time": "2023-09-13T06:17:42.390299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A1', 11), ('A2', 12), ('A3', 13), ('A4', 14)]\n",
      "[('A1', 11), ('A2', 12), ('A3', 13), ('A4', 14)]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('A1', 11), ('A2', 12), ('A3', 13), ('A4', 14)])\n",
    "rdd1_broadcast = sc.broadcast(rdd1.collect())\n",
    "print(rdd1.collect())\n",
    "print(rdd1_broadcast.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7178fc",
   "metadata": {},
   "source": [
    "## 资源参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c048a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T06:19:37.419694Z",
     "start_time": "2023-09-13T06:19:37.413614Z"
    }
   },
   "source": [
    "如果要进行资源调优，我们就必须先知道Spark运行的机制与流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd49e6",
   "metadata": {},
   "source": [
    "![spark](spark.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38389f2d",
   "metadata": {},
   "source": [
    "### num-executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecd6b1",
   "metadata": {},
   "source": [
    "指的是执行器的数量，数量的多少代表了并行的stage数量（假如executor是单核的话），但也并不是越多越快，受你集群资源的限制，所以一般设置50-100左右吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4638bd",
   "metadata": {},
   "source": [
    "### executor-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2716d",
   "metadata": {},
   "source": [
    "这里指的是每一个执行器的内存大小，内存越大当然对于程序运行是很好的了，但是也不是无节制地大下去，同样受我们集群资源的限制。假设我们集群资源为500core，一般1core配置4G内存，所以集群最大的内存资源只有2000G左右。num-executors x executor-memory 是不能超过2000G的，但是也不要太接近这个值，不然的话集群其他同事就没法正常跑数据了，一般我们设置4G-8G。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4a297",
   "metadata": {},
   "source": [
    "### executor-cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7207b4d",
   "metadata": {},
   "source": [
    "这里设置的是executor的CPU core数量，决定了executor进程并行处理task的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cecd57",
   "metadata": {},
   "source": [
    "### driver-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9cff1",
   "metadata": {},
   "source": [
    "设置driver的内存，一般设置2G就好了。但如果想要做一些Python的DataFrame操作可以适当地把这个值设大一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930041f6",
   "metadata": {},
   "source": [
    "### driver-cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a0cf0",
   "metadata": {},
   "source": [
    "与executor-cores类似的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a9015",
   "metadata": {},
   "source": [
    "### spark.default.parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc04df",
   "metadata": {},
   "source": [
    "设置每个stage的task数量。一般Spark任务我们设置task数量在500-1000左右比较合适，如果不去设置的话，Spark会根据底层HDFS的block数量来自行设置task数量。有的时候会设置得偏少，这样子程序就会跑得很慢，即便你设置了很多的executor，但也没有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfaecc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247.365px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
