{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a484336",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" color=green size=5>SparkDataFrame基础</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79f86b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T09:06:19.417077Z",
     "start_time": "2024-11-18T09:06:12.423688Z"
    },
    "execution": {
     "iopub.execute_input": "2024-08-13T07:19:24.139555Z",
     "iopub.status.busy": "2024-08-13T07:19:24.138567Z",
     "iopub.status.idle": "2024-08-13T07:19:29.503106Z",
     "shell.execute_reply": "2024-08-13T07:19:29.502204Z",
     "shell.execute_reply.started": "2024-08-13T07:19:24.139063Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import findspark\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "spark_home = \"D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\pyspark\"\n",
    "python_path = \"D:\\\\Anaconda\\\\python\"\n",
    "findspark.init(spark_home,python_path)\n",
    "spark = SparkSession.builder.appName('SparkDataFrame').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0376257",
   "metadata": {},
   "source": [
    "# 读取SparkDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9119d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T06:27:45.593435Z",
     "start_time": "2024-09-23T06:27:40.655820Z"
    }
   },
   "outputs": [],
   "source": [
    "df_iris = spark.read.csv(\"../data/iris.csv\", header=True, inferSchema=True)\n",
    "#header参数设置为True，表示第一行包含列名。inferSchema参数设置为True，表示Spark将尝试推断每列的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca46cf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:29:00.745527Z",
     "start_time": "2023-11-06T08:29:00.739501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepallength: double (nullable = true)\n",
      " |-- sepalwidth: double (nullable = true)\n",
      " |-- petallength: double (nullable = true)\n",
      " |-- petalwidth: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.printSchema() #查看数据的结构和列的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c701f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:29:04.023024Z",
     "start_time": "2023-11-06T08:29:03.999354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepallength: double (nullable = true)\n",
      " |-- sepalwidth: double (nullable = true)\n",
      " |-- petallength: double (nullable = true)\n",
      " |-- petalwidth: double (nullable = true)\n",
      " |-- label: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris = df_iris.withColumn(\"label\", df_iris[\"label\"].cast(\"float\"))#转换数据类型\n",
    "df_iris.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440768e",
   "metadata": {},
   "source": [
    "# 创建SparkDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e37843",
   "metadata": {},
   "source": [
    "## 使用RDD来创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f3257",
   "metadata": {},
   "source": [
    "主要使用RDD的toDF方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3718971c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T09:26:03.339327Z",
     "start_time": "2023-10-27T09:25:32.910448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+\n",
      "| name|age|score|\n",
      "+-----+---+-----+\n",
      "|  Sam| 28|   88|\n",
      "|Flora| 28|   90|\n",
      "|  Run|  1|   60|\n",
      "+-----+---+-----+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Sam\", 28, 88), (\"Flora\", 28, 90), (\"Run\", 1, 60)] #list中的每一个元素都是元组\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "dfFromRDD1 = rdd.toDF([\"name\", \"age\", \"score\"])\n",
    "dfFromRDD1.show()\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a7d0e",
   "metadata": {},
   "source": [
    "## 使用python的DataFrame来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d64df2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:39:02.496697Z",
     "start_time": "2023-11-08T07:38:48.177778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name  age  score\n",
      "0    Sam   28     88\n",
      "1  Flora   28     90\n",
      "2    Run    1     60\n",
      "+-----+---+-----+\n",
      "| name|age|score|\n",
      "+-----+---+-----+\n",
      "|  Sam| 28|   88|\n",
      "|Flora| 28|   90|\n",
      "|  Run|  1|   60|\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第一种\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([['Sam', 28, 88], ['Flora', 28, 90], ['Run', 1, 60]],\n",
    "                  columns=['name', 'age', 'score'])\n",
    "print(df)\n",
    "Spark_df = spark.createDataFrame(df)\n",
    "Spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87c4d589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:40:43.269189Z",
     "start_time": "2023-11-08T07:40:28.185315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+\n",
      "| name|age|score|\n",
      "+-----+---+-----+\n",
      "|  Sam| 28|   88|\n",
      "|Flora| 28|   90|\n",
      "|  Run|  1|   60|\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第二种\n",
    "import pyspark.pandas as ps\n",
    "Spark_df = ps.from_pandas(df).to_spark()\n",
    "Spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86204a08",
   "metadata": {},
   "source": [
    "## 使用pyspark.pandas来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a583336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:13:57.567934Z",
     "start_time": "2023-11-08T07:13:57.498622Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import findspark\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "list_values = [['Sam', 28, 88], ['Flora', 28, 90], ['Run', 1, 60]]\n",
    "Spark_df = ps.DataFrame(data=list_values, columns=['name', 'age', 'score']).to_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af4f55f",
   "metadata": {},
   "source": [
    "### 创建空DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "042b3f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:15:13.369745Z",
     "start_time": "2023-11-08T07:15:13.307742Z"
    }
   },
   "outputs": [],
   "source": [
    "a = ps.DataFrame(data=[], columns=['name', 'age', 'score']).to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5399d975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:15:29.363233Z",
     "start_time": "2023-11-08T07:15:15.003637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea13c7",
   "metadata": {},
   "source": [
    "## 使用List来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b5332f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T02:08:13.288323Z",
     "start_time": "2024-03-21T02:07:56.875591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+\n",
      "| name|age|score|\n",
      "+-----+---+-----+\n",
      "|  Sam| 28|   88|\n",
      "|Flora| 28|   90|\n",
      "|  Run|  1| null|\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_values = [['Sam', 28, 88], ['Flora', 28, 90], ['Run', 1, None]]\n",
    "Spark_df = spark.createDataFrame(list_values, ['name', 'age', 'score'])\n",
    "Spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd78715",
   "metadata": {},
   "source": [
    "## 读取数据文件来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f5cda01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T01:46:27.964692Z",
     "start_time": "2024-03-21T01:46:27.627305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----+\n",
      "|sepallength|sepalwidth|petallength|petalwidth|label|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "|        5.1|       3.5|        1.4|      null|    0|\n",
      "|       null|       3.0|        1.4|       0.2|    0|\n",
      "|        4.7|       3.2|       null|       0.2|    0|\n",
      "|        4.6|       3.1|       null|       0.2|    0|\n",
      "|        5.0|       3.6|        1.4|       0.2|    0|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- sepallength: double (nullable = true)\n",
      " |-- sepalwidth: double (nullable = true)\n",
      " |-- petallength: double (nullable = true)\n",
      " |-- petalwidth: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .csv(\"../data/iris.csv\")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c7ad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T08:43:57.566519Z",
     "start_time": "2024-08-13T08:43:57.562308Z"
    }
   },
   "source": [
    "spark.read.csv(path=data, encoding='gbk', quote='\"', header='True', multiLine=True)\n",
    "- path:文件名\n",
    "- encoding:编码方式\n",
    "- quote:忽略数据中的双引号，如果不忽略会错误解析\n",
    "- header:是否有表头\n",
    "- multiLine:允许字段跨越多行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a2491",
   "metadata": {},
   "source": [
    "## 通过读取数据库来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf2ac669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T06:30:25.295337Z",
     "start_time": "2024-08-13T06:30:24.960480Z"
    },
    "execution": {
     "iopub.execute_input": "2024-08-13T07:24:50.080256Z",
     "iopub.status.busy": "2024-08-13T07:24:50.076821Z",
     "iopub.status.idle": "2024-08-13T07:24:50.186329Z",
     "shell.execute_reply": "2024-08-13T07:24:50.184966Z",
     "shell.execute_reply.started": "2024-08-13T07:24:50.080256Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# url = \"jdbc:mysql://192.168.11.138:3306/ai_monitor\"\n",
    "url = \"jdbc:mysql://localhost:3306/sys\"\n",
    "\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    " .option(\"url\", url) \\\n",
    " .option(\"dbtable\", \"data_gt1_cms_201508_201604\") \\\n",
    " .option(\"user\", \"root\") \\\n",
    " .option(\"password\", \"root\") \\\n",
    " .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73b9bc38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T06:30:33.035672Z",
     "start_time": "2024-08-13T06:30:27.729431Z"
    },
    "execution": {
     "iopub.execute_input": "2024-08-13T07:24:54.815359Z",
     "iopub.status.busy": "2024-08-13T07:24:54.814359Z",
     "iopub.status.idle": "2024-08-13T07:24:58.897076Z",
     "shell.execute_reply": "2024-08-13T07:24:58.895218Z",
     "shell.execute_reply.started": "2024-08-13T07:24:54.814359Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|            ts|      DWATT|       CSGV|       CTIM|    TTXD1_1|    TTXD1_2|    TTXD1_3|    TTXD1_4|    TTXD1_5|    TTXD1_6|    TTXD1_7|    TTXD1_8|    TTXD1_9|   TTXD1_10|   TTXD1_11|   TTXD1_12|   TTXD1_13|   TTXD1_14|   TTXD1_15|   TTXD1_16|   TTXD1_17|   TTXD1_18|   TTXD1_19|   TTXD1_20|   TTXD1_21|   TTXD1_22|   TTXD1_23|   TTXD1_24|       TTXM|     TTXSP1|     TTXSP3|     TTXSP2|\n",
      "+--------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "| 2015/8/1 0:00|103.0826721|84.05797577|91.35520935|1056.271606|1056.249512| 1056.90564| 1039.57605|1051.324463|1071.620239|1051.213257|1055.065552|1064.045776|1062.151123|1049.986206|1051.356934|1048.247192|1043.951538|1061.442871|1064.633301|1062.626587|1042.492554|1040.360107|1044.769287|1061.994507|1052.054688|1055.862671|1061.955322|1055.949707|25.66916656|23.58292007|24.93045998|\n",
      "| 2015/8/1 1:00|103.3062134|84.06842041|89.63658905|1057.366943|1055.130981|1057.074707|1037.682373|1049.881348| 1070.99231| 1050.53772|1053.835083|1061.643188|1061.310913|1050.171021|1049.965088| 1046.13855| 1041.01123|1061.208862|1062.317383|  1060.7677|1040.384399|1040.169312|1042.369751|1060.376343|1050.632324|1053.875122|1060.029053|1052.199707|25.02660942|22.51792336|23.52773857|\n",
      "| 2015/8/1 2:00|103.5618591|84.05568695|89.13874054|1055.688599|1053.412842|1056.954224|1039.199585|1050.531128|1071.470703|1051.274048|1055.597046|1064.537231|1060.905518|1051.084106|1053.052368|1047.559814|1043.053833|1060.550903|1062.386719|1059.970703|1041.285522|1040.676514|1041.763428|1062.228394|1050.305054|1056.360107|1059.859985|1054.869141| 26.2706337|22.97101974|22.13184357|\n",
      "| 2015/8/1 3:00|103.5441971| 84.0637207|88.77116394|1056.649536|1054.690308|1058.509521|1037.749512|1047.575073|1069.485229|1049.310425|1054.250366|1062.025757|1062.128906|1047.870483|1050.330078|1047.552734|1041.790039|1061.594482|1061.341553| 1059.89917|1040.759644|1039.704102|1042.206299|1060.811523|1051.064087|1054.896729|1059.316284|1053.093628|25.26716232|22.15986633|22.32428169|\n",
      "| 2015/8/1 4:00|77.35246277|61.77230453|97.53249359|1093.326172| 1090.55542|1087.083008|1088.796143|1083.110962|1088.895142|1079.944946|1098.591064|1095.516479|1092.768677|1096.011108|1088.755859|1084.335693|1088.131592|  1076.7677|1087.193115|1090.641968| 1088.12854|1085.520508|1087.992065|1087.710327|1075.902954|  1095.7323|1092.937622|1088.570313|22.87000656|17.25897789|20.66464233|\n",
      "| 2015/8/1 5:00|99.55996704|76.90935516|89.04841614|1062.718506|1064.148193|1063.577393|1048.179443|1061.395508|1071.485962|1057.560913|1067.901367|1075.404907|1070.370728|1059.484131|1062.705078|1057.109497|1051.845337|1064.881104|1074.899414|1065.170044|1051.699219|1043.226318|1055.401611|1074.675171|1063.646606|1065.093872|1071.851807|1063.424683|31.84046364|23.65743446|24.02619743|\n",
      "| 2015/8/1 6:00|97.68922424|70.59851837|88.01203918|1074.510986|1075.490723|1074.996948|1060.171753|1073.660034|1082.450317| 1067.11377|1080.100708|1084.793823|1078.767822|1067.577515|1072.877563|1067.697144|1059.853638|1075.344116|1087.257813|1076.148804|1064.356567|1053.265015|1069.015381|1087.710449|1075.852661|1074.234375|1083.049194|1073.468506|32.25344467|26.38257217|25.97006798|\n",
      "| 2015/8/1 7:00|96.79737854|69.42015839|88.10221863|1076.403687|1077.287842|1076.689575|1061.080811|1075.489746|1084.945068|1069.192261|1083.048706| 1086.73291|1079.815308|1069.503052|1075.247192|1069.454956|1059.979492|1075.793579|1088.822754| 1078.66394|1066.004517|1055.456055|1069.943115|1087.344727| 1076.34314|1077.671753|1083.229858|1073.942261|30.49998474|24.03955078|24.91410828|\n",
      "| 2015/8/1 8:00|83.68569946|61.65276337|97.28492737| 1088.29541|1077.193359|1084.321777|1079.157715|1077.550903|1081.873779|1073.943359|1095.380737|1088.372803| 1088.08252|1088.931641|1079.715942|1082.104492|1078.347168|1068.622314|1085.305908|   1086.125|1082.924561|1076.780273|1082.769775|1079.144531|1067.054199|1090.906616|1086.246948|1082.471313|24.71356773|18.85206795|23.95149612|\n",
      "| 2015/8/1 9:00|96.31951904|72.83722687|91.25266266|1069.043579|1072.265503|1071.861572|1058.124756|1072.440063|1079.980957|1064.648804|1077.575073|1079.464722|1074.698853|1063.286011|1070.514404|1065.425659|1056.900024|1069.817261|1082.532227|1073.086426|1059.331177|1050.714966|1067.057617|1080.760132|1069.369751|1073.343506|1078.542358|1069.952148|32.10974121| 22.9158287|23.60400963|\n",
      "|2015/8/1 10:00|99.93180084|78.27552795|91.98561096| 1060.09436|1063.139771|1061.923706|1045.998413|1058.895142| 1071.17749| 1053.93396|1062.990479|1071.171997|1067.005493|1056.134033|1058.097534|1053.477417|1047.726318|1064.051514|1071.746582|1061.369141|1047.870605|1042.042969|1053.300415|1071.106934|1060.310303| 1060.46814|1070.912842| 1059.86792|31.73847389|25.59329987|26.03712273|\n",
      "|2015/8/1 11:00|100.9497986| 84.0184021| 94.9950943|1059.715698|1060.160156|1057.902222|1042.743164|1053.133789|1071.834839| 1054.05957|1059.538086|1067.010742|1065.368164|1053.056519|1055.773926|1050.683594|1044.439087| 1063.27002|1065.933228|1059.370605|1045.275513|1041.458374|1045.807251|1064.876587|1055.155029|1058.237793| 1065.18335|1056.640015|25.42960548|21.83244896|25.81059647|\n",
      "|2015/8/1 12:00|99.52737427|84.05713654|97.33135986| 1060.70752|1061.253906|1059.400146|1042.526489|1054.454224|1070.254395|1054.613403|1060.307983|1067.244385|1065.510132| 1053.53064|1054.521362|1051.013306|1045.440308|1064.105713| 1068.08374|1063.804932|1043.396362|1042.150391|1048.079346|1068.348389|1057.570923|1059.346558|1066.735962|1058.536621|27.45727539|24.19117928| 26.2739315|\n",
      "|2015/8/1 13:00|98.04136658|84.05736542|98.35830688|1060.444214|1061.359375|1060.331787| 1043.24939|1055.437866|1069.068237|1053.788696|1059.849487|1068.138916|1064.750854|1053.656982|1055.759644|1051.852173|1045.781616|1062.660034|1069.197021|1064.524902|1045.414307|1042.132813|1048.471924|1069.829346|1058.973999|1060.647095|1067.822144|1058.762085|27.66268921|22.88910103|25.84186935|\n",
      "|2015/8/1 14:00|97.58633423|84.07151794|100.6073685|1062.635132|1062.226685|1059.544434|1043.864258|1056.574951|1069.384155|1054.960571|1060.937744|1068.736328|1065.420288|1055.187866|1056.141113|1054.031982|1047.443726|1064.009399|1070.737549|1068.040161|1046.091064|1041.118774|1049.200684|1070.989868|1060.798828|1061.190552|1070.493286|1059.335693|27.06142044|25.28053856|26.16587067|\n",
      "|2015/8/1 15:00|97.79223633|84.06069946|102.1710129|1062.515381|1062.721924|1060.074951| 1047.25293| 1058.25769|1068.900635|1054.854858|1060.873169|1068.772095|1064.885498|1054.967896|1057.521606|1055.172607|1046.227051|1063.651367|1069.927002| 1066.38855|1047.569458|1042.158936|1050.501831|1071.970093|1060.440063|1062.115723|1070.403076|1059.337158|27.78708839|25.73071861|24.76664543|\n",
      "|2015/8/1 16:00|96.87033081|84.06354523| 102.053215| 1061.77063|1062.810669|1061.511475|1044.724976|1057.644043|1070.334839|1054.687134|1062.912476|1069.852783|1067.074829| 1055.74939| 1057.79187|1053.434326|1048.099976|1064.634644|1071.028564|1067.306274|1046.340088|1042.577515|1049.705078|1071.663696|1061.236328|1062.756226|1070.348145|1059.603638| 28.8423214|23.47604942|25.54654312|\n",
      "|2015/8/1 17:00|96.83037567|  80.097229|100.8692932|1066.802124|1067.711548|1066.775757|1051.527954|1064.478149|1074.358154|1058.905273|1069.741455|1074.695801|1070.280151|1059.798218|1064.290039|1059.827393|1052.154419|1067.001465|1077.188965|1066.022583|1054.726074|1044.517822|1056.213745|1077.411621|1066.842896|1066.696167|1073.048462|1065.130615|31.66178703|23.16365433|25.41536522|\n",
      "|2015/8/1 18:00|99.41722107|84.06088257|98.00387573|1057.973022| 1059.71814|1060.509766|1042.687012|1054.882813|1070.490479|1053.615723|1060.372803|1067.974121|1063.965332|1053.432495|1055.424683|1051.306152|1046.091797|1062.531982|1068.513428|1062.074341|1046.707764|1041.940918|1047.029053|1065.910522|1057.773804|1060.011108|1066.387329|1056.323242|26.11506081|23.97734833|24.24050331|\n",
      "|2015/8/1 19:00|95.91436005|73.93493652|96.23653412| 1073.25415|1073.807129|1072.234741|1058.439453|1071.503418| 1079.35791|1064.842163|1077.011108|1081.734131|1076.537476| 1065.58667|1071.305542|1067.097778|1057.911865| 1072.66626|1084.940186|1074.015381|1062.103027|1051.410767|1066.380493|1084.726074|1074.739746|1072.449585|1080.552612|1072.084595|30.37875557|25.66559029|26.15875244|\n",
      "+--------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99764ecb",
   "metadata": {},
   "source": [
    "# 保存SparkDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb281c53",
   "metadata": {},
   "source": [
    "在将DataFrame保存为CSV文件时，需要注意以下事项：\n",
    "\n",
    "- 输出文件夹或文件路径必须是一个不存在的路径，否则将会抛出错误。\n",
    "- 如果输出文件夹已经存在，Spark会自动为保存的CSV文件添加后缀以避免文件名冲突。\n",
    "- 默认情况下，每个CSV文件的文件名将为part-00000、part-00001等，可以在write.csv方法中通过option(\"pathGlobFilter\", \"*.csv\")设置文件名。\n",
    "- 如果DataFrame包含NULL值，则在保存为CSV文件时，NULL值将被保存为空字符串。\n",
    "- 如果DataFrame的列中包含特殊字符或引号，记得在保存CSV文件时使用适当的转义字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a307630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:37:20.573477Z",
     "start_time": "2023-11-13T03:37:20.089538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----+\n",
      "|sepallength|sepalwidth|petallength|petalwidth|label|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "|        5.1|       3.5|        1.4|       0.2|    0|\n",
      "|        4.9|       3.0|        1.4|       0.2|    0|\n",
      "|        4.7|       3.2|        1.3|       0.2|    0|\n",
      "|        4.6|       3.1|        1.5|       0.2|    0|\n",
      "|        5.0|       3.6|        1.4|       0.2|    0|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris = spark.read.csv(\"../data/iris.csv\", header=True, inferSchema=True)\n",
    "df_iris.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d705ddd",
   "metadata": {},
   "source": [
    "## 保存DataFrame为多个CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a305df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:38:43.015612Z",
     "start_time": "2023-11-13T03:38:42.714939Z"
    }
   },
   "outputs": [],
   "source": [
    "save_path = \"../data/iris\"\n",
    "df_iris.write.csv(save_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.read.csv(save_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2eb2f4",
   "metadata": {},
   "source": [
    "## 保存DataFrame为单个CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d4a41c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:39:54.721718Z",
     "start_time": "2023-11-13T03:39:54.549938Z"
    }
   },
   "outputs": [],
   "source": [
    "save_path = \"../data/iris_single\"\n",
    "df_iris.coalesce(1).write.csv(save_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137499b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.read.csv(save_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab750968",
   "metadata": {},
   "source": [
    "# 常用的SparkDataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62f215",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d410e1b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T02:09:42.497487Z",
     "start_time": "2024-10-14T02:09:08.155338Z"
    },
    "execution": {
     "iopub.execute_input": "2024-06-13T11:47:29.951650Z",
     "iopub.status.busy": "2024-06-13T11:47:29.950645Z",
     "iopub.status.idle": "2024-06-13T11:47:43.389097Z",
     "shell.execute_reply": "2024-06-13T11:47:43.388134Z",
     "shell.execute_reply.started": "2024-06-13T11:47:29.951650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+----+\n",
      "|  name| age|score| sex|\n",
      "+------+----+-----+----+\n",
      "|   Sam|  28| 88.0|   M|\n",
      "| Flora|  28| 90.0|   F|\n",
      "|   Run|   1| 60.0|null|\n",
      "| Peter|  55|100.0|   M|\n",
      "|   Mei|  54| 95.0|   F|\n",
      "|   Lei|  59| 65.0|   F|\n",
      "|  Sam1|  28| 88.0|   M|\n",
      "|Flora1|  28| 90.0|   F|\n",
      "|  Run1|   1| 60.0|null|\n",
      "|Peter1|null| null|   M|\n",
      "|  Mei1|  54| 95.0|   F|\n",
      "|  Lei1|  59| 65.0|   F|\n",
      "+------+----+-----+----+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建一个SparkDataFrame\n",
    "rdd = spark.sparkContext.parallelize([(\"Sam\", 28, 88., \"M\"),\n",
    "                                      (\"Flora\", 28, 90., \"F\"),\n",
    "                                      (\"Run\", 1, 60., None),\n",
    "                                      (\"Peter\", 55, 100., \"M\"),\n",
    "                                      (\"Mei\", 54, 95., \"F\"),\n",
    "                                      (\"Lei\", 59, 65., \"F\"),\n",
    "                                      (\"Sam1\", 28, 88., \"M\"),\n",
    "                                      (\"Flora1\", 28, 90., \"F\"),\n",
    "                                      (\"Run1\", 1, 60., None),\n",
    "                                      (\"Peter1\", 55., 100, \"M\"),\n",
    "                                      (\"Mei1\", 54, 95., \"F\"),\n",
    "                                      (\"Lei1\", 59, 65., \"F\")])\n",
    "df = rdd.toDF([\"name\", \"age\", \"score\", \"sex\"])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabba226",
   "metadata": {},
   "source": [
    "## DataFrame的APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab558c",
   "metadata": {},
   "source": [
    "### 查看类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cda9208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T02:20:24.060544Z",
     "start_time": "2024-03-22T02:20:24.055538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'bigint'), ('score', 'double'), ('sex', 'string')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a67df1df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T03:10:58.531055Z",
     "start_time": "2024-03-22T03:10:58.517115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructField('name', StringType(), True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('name').schema[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaaabcf",
   "metadata": {},
   "source": [
    "### 修改类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145d07e",
   "metadata": {},
   "source": [
    "类型：'string'、'double'、'int'、'timestamp'、'boolean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f66be49a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T02:16:33.787098Z",
     "start_time": "2024-03-22T02:16:33.772483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('name').dtypes[0][1] == 'string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc5267f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T02:20:50.850510Z",
     "start_time": "2024-03-22T02:20:34.177192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+----+\n",
      "|  name| age|score| sex|\n",
      "+------+----+-----+----+\n",
      "|   Sam|28.0| 88.0|   M|\n",
      "| Flora|28.0| 90.0|   F|\n",
      "|   Run| 1.0| 60.0|null|\n",
      "| Peter|55.0|100.0|   M|\n",
      "|   Mei|54.0| 95.0|   F|\n",
      "|   Lei|59.0| 65.0|   F|\n",
      "|  Sam1|28.0| 88.0|   M|\n",
      "|Flora1|28.0| 90.0|   F|\n",
      "|  Run1| 1.0| 60.0|null|\n",
      "|Peter1|null| null|   M|\n",
      "|  Mei1|54.0| 95.0|   F|\n",
      "|  Lei1|59.0| 65.0|   F|\n",
      "+------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"age\", F.col(\"age\").cast(\"double\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa678630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改为时间列\n",
    "df.withColumn(time_col, F.to_timestamp(F.col(time_col), 'yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d192d",
   "metadata": {},
   "source": [
    "### 查询行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35fdf463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:30:23.790845Z",
     "start_time": "2023-11-06T08:29:55.687823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n",
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "+-----+---+-----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#指定要打印的行数\n",
    "df.show()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25bcf7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T06:11:00.506339Z",
     "start_time": "2024-03-19T06:10:42.172182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Sam', age=28, score=88, sex='M'),\n",
       " Row(name='Flora', age=28, score=90, sex='F'),\n",
       " Row(name='Run', age=1, score=60, sex=None),\n",
       " Row(name='Peter', age=55, score=100, sex='M'),\n",
       " Row(name='Mei', age=54, score=95, sex='F')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以列表形式返回行\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cf91f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T09:45:27.744329Z",
     "start_time": "2023-10-27T09:45:13.025971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查询总行数\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4243118f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:30:33.158872Z",
     "start_time": "2023-11-06T08:30:28.603469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Sam', age=28, score=88, sex='M')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看第1条数据\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dfb7106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:30:56.274928Z",
     "start_time": "2023-11-06T08:30:33.158872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Sam', age=28, score=88, sex='M'),\n",
       " Row(name='Flora', age=28, score=90, sex='F')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取头几行到本地\n",
    "df.head(2)\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "684c1317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T09:51:11.937208Z",
     "start_time": "2024-03-20T09:51:07.640662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Haffman', department='Marketing', salary=7000),\n",
       " Row(name='Ilaja', department='Marketing', salary=8000),\n",
       " Row(name='Joey', department='Sales', salary=9000)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取后几行\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558cf62b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:31:10.532299Z",
     "start_time": "2023-11-06T08:30:56.276921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Flora| 28|   90|  F|\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 按照一定规则从df随机抽样数据\n",
    "df.sample(0.5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5943df",
   "metadata": {},
   "source": [
    "### 查询列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e3e927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T02:21:13.524218Z",
     "start_time": "2024-03-19T02:21:13.516517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'score', 'sex']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查询列名\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5255ee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T02:21:49.543259Z",
     "start_time": "2024-03-19T02:21:49.537379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex', 'age', 'score']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(df.columns) - set(['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcd1bd09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:02:00.416229Z",
     "start_time": "2023-10-27T10:02:00.410725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'bigint'), ('score', 'bigint'), ('sex', 'string')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查询列类型\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94f2b636",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:00:18.477003Z",
     "start_time": "2023-10-27T10:00:03.521868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|                 5|\n",
      "|   mean|              33.2|\n",
      "| stddev|22.353970564532826|\n",
      "|    min|                 1|\n",
      "|    max|                55|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 返回列的基础统计信息\n",
    "df.describe(['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf4a1336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T09:59:56.858664Z",
     "start_time": "2023-10-27T09:59:41.501619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+------------------+----+\n",
      "|summary| name|               age|             score| sex|\n",
      "+-------+-----+------------------+------------------+----+\n",
      "|  count|    5|                 5|                 5|   4|\n",
      "|   mean| null|              33.2|              86.6|null|\n",
      "| stddev| null|22.353970564532826|15.582040944625966|null|\n",
      "|    min|Flora|                 1|                60|   F|\n",
      "|    max|  Sam|                55|               100|   M|\n",
      "+-------+-----+------------------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查询概况\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1c7d69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:00:36.905121Z",
     "start_time": "2023-10-27T10:00:22.062117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| sex|score|\n",
      "+----+-----+\n",
      "|   M|   88|\n",
      "|   F|   90|\n",
      "|null|   60|\n",
      "|   M|  100|\n",
      "|   F|   95|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 选定指定列并按照一定顺序呈现\n",
    "df.select(\"sex\", \"score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef89aa56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:02:23.877539Z",
     "start_time": "2023-10-27T10:02:08.862474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|  age_freqItems|sex_freqItems|\n",
      "+---------------+-------------+\n",
      "|[55, 1, 28, 54]| [M, null, F]|\n",
      "+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看指定列的枚举值\n",
    "df.freqItems([\"age\",\"sex\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "932da64b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:03:03.443367Z",
     "start_time": "2023-10-27T10:02:47.906392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+------------------+----+\n",
      "|summary| name|               age|             score| sex|\n",
      "+-------+-----+------------------+------------------+----+\n",
      "|  count|    5|                 5|                 5|   4|\n",
      "|   mean| null|              33.2|              86.6|null|\n",
      "| stddev| null|22.353970564532826|15.582040944625966|null|\n",
      "|    min|Flora|                 1|                60|   F|\n",
      "|    25%| null|                28|                88|null|\n",
      "|    50%| null|                28|                90|null|\n",
      "|    75%| null|                54|                95|null|\n",
      "|    max|  Sam|                55|               100|   M|\n",
      "+-------+-----+------------------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60291f7",
   "metadata": {},
   "source": [
    "### 选择行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cfa210",
   "metadata": {},
   "source": [
    "#### monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b8aff25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:16:30.753382Z",
     "start_time": "2023-10-27T10:16:15.866583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------+\n",
      "| name|((id >= 2) AND (id <= 4))|\n",
      "+-----+-------------------------+\n",
      "|  Sam|                    false|\n",
      "|Flora|                    false|\n",
      "|  Run|                    false|\n",
      "|Peter|                    false|\n",
      "|  Mei|                    false|\n",
      "+-----+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#选择dataframe中间的特定行数，该函数是不确定的，因为它的结果取决于分区 ID。\n",
    "from pyspark.sql.functions import monotonically_increasing_id #设定自增长列\n",
    "dfWithIndex = df.withColumn('id',monotonically_increasing_id()) #添加自增长列\n",
    "dfWithIndex.select(dfWithIndex.name, dfWithIndex.id.between(2, 4)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130992a",
   "metadata": {},
   "source": [
    "#### row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy(F.lit(1))\n",
    "df = df.withColumn(\"ID\", F.row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d488e",
   "metadata": {},
   "source": [
    "### 选择列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98896fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T06:55:03.848284Z",
     "start_time": "2024-03-19T06:54:47.750043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Sam', age=28),\n",
       " Row(name='Flora', age=28),\n",
       " Row(name='Run', age=1),\n",
       " Row(name='Peter', age=55),\n",
       " Row(name='Mei', age=54)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = df.select('name','age')\n",
    "dfs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43b826f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T09:39:00.112856Z",
     "start_time": "2024-03-19T09:38:44.185730Z"
    }
   },
   "outputs": [],
   "source": [
    "age = df.select('age').collect()\n",
    "list_age = [i[0] for i in age]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f241344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T09:39:08.275743Z",
     "start_time": "2024-03-19T09:39:08.265959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 28, 1, 55, 54]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bc557e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:01:03.844611Z",
     "start_time": "2024-03-19T07:00:47.806949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.where(F.col('name') == 'Mei').collect()[0]['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b9df94d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T08:07:49.571571Z",
     "start_time": "2024-03-18T08:07:33.446406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('age').collect()[4]['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63fe00f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T01:42:51.342771Z",
     "start_time": "2024-07-30T01:42:51.260262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['score'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择符合类型的列\n",
    "df1 = df.pandas_api()\n",
    "df1.select_dtypes(include=['float']).columns # 包含"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57ba4c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T01:44:08.697271Z",
     "start_time": "2024-07-30T01:44:08.521438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'score'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择符合类型的列\n",
    "df2 = df.pandas_api()\n",
    "df2.select_dtypes(exclude=['string', 'datetime', 'datetime64', 'bool']).columns # 不包含"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f534b2",
   "metadata": {},
   "source": [
    "### 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2d4271f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:06:17.750357Z",
     "start_time": "2023-10-27T10:06:03.250457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#降序\n",
    "df.orderBy(df['age'].desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eb89bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T06:13:48.764275Z",
     "start_time": "2024-03-20T06:13:32.807278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Peter| 55|  100|  M|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 降序只取一条\n",
    "df.orderBy(df['age'].desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c76aba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:07:50.753435Z",
     "start_time": "2023-10-27T10:07:35.450035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Run|  1|   60|null|\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Mei| 54|   95|   F|\n",
      "|Peter| 55|  100|   M|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#升序\n",
    "df.orderBy(df['age']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73a47af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T03:47:12.395491Z",
     "start_time": "2024-07-17T03:46:55.437594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+----+\n",
      "|  name| age|score| sex|\n",
      "+------+----+-----+----+\n",
      "|Peter1|null| null|   M|\n",
      "|   Run|   1| 60.0|null|\n",
      "|  Run1|   1| 60.0|null|\n",
      "| Flora|  28| 90.0|   F|\n",
      "|   Sam|  28| 88.0|   M|\n",
      "|  Sam1|  28| 88.0|   M|\n",
      "|Flora1|  28| 90.0|   F|\n",
      "|  Mei1|  54| 95.0|   F|\n",
      "|   Mei|  54| 95.0|   F|\n",
      "| Peter|  55|100.0|   M|\n",
      "|   Lei|  59| 65.0|   F|\n",
      "|  Lei1|  59| 65.0|   F|\n",
      "+------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 升序\n",
    "df.orderBy(['age'], ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39762af8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T03:49:12.091132Z",
     "start_time": "2024-07-17T03:48:55.123741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+----+\n",
      "|  name| age|score| sex|\n",
      "+------+----+-----+----+\n",
      "|Peter1|null| null|   M|\n",
      "|  Run1|   1| 60.0|null|\n",
      "|   Run|   1| 60.0|null|\n",
      "| Flora|  28| 90.0|   F|\n",
      "|Flora1|  28| 90.0|   F|\n",
      "|  Sam1|  28| 88.0|   M|\n",
      "|   Sam|  28| 88.0|   M|\n",
      "|  Mei1|  54| 95.0|   F|\n",
      "|   Mei|  54| 95.0|   F|\n",
      "| Peter|  55|100.0|   M|\n",
      "|   Lei|  59| 65.0|   F|\n",
      "|  Lei1|  59| 65.0|   F|\n",
      "+------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分组排序，先对age升序，再对socre分组降序\n",
    "sort_columns = ['age', 'score']\n",
    "ascending_columns = [True, False]\n",
    "df.orderBy(sort_columns, ascending=ascending_columns).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8db093",
   "metadata": {},
   "source": [
    "### 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53cb3ef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:24:39.932692Z",
     "start_time": "2023-10-27T10:24:25.456089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对数据集进行去重\n",
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de18df74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:48:18.103261Z",
     "start_time": "2023-10-30T01:48:02.264357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Run|  1|   60|null|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Sam| 28|   88|   M|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对指定列去重\n",
    "df.dropDuplicates([\"sex\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b46150e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:49:53.588729Z",
     "start_time": "2023-10-30T01:48:22.779681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据指定的df对df进行去重\n",
    "df1 = spark.createDataFrame(\n",
    "        [(\"a\", 1), (\"a\", 1), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
    "df3 = df1.exceptAll(df2)  # 没有去重的功效\n",
    "df4 = df1.subtract(df2)  # 有去重的奇效\n",
    "df1.show()\n",
    "df2.show()\n",
    "df3.show()\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd93d5e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:50:23.687837Z",
     "start_time": "2023-10-30T01:49:53.592722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 返回两个DataFrame的交集\n",
    "df1 = spark.createDataFrame(\n",
    "        [(\"a\", 1), (\"a\", 1), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 4)], [\"C1\", \"C2\"])\n",
    "df1.intersectAll(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a184f0d",
   "metadata": {},
   "source": [
    "### 丢弃列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d5cac8ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:50:50.099453Z",
     "start_time": "2023-10-30T01:50:35.018185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| name|score| sex|\n",
      "+-----+-----+----+\n",
      "|  Sam|   88|   M|\n",
      "|Flora|   90|   F|\n",
      "|  Run|   60|null|\n",
      "|Peter|  100|   M|\n",
      "|  Mei|   95|   F|\n",
      "+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 丢弃指定列\n",
    "df.drop('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "385345f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:52:25.712927Z",
     "start_time": "2023-10-30T01:52:10.491577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|  Sam| 28|   88|  M|\n",
      "|Flora| 28|   90|  F|\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 丢弃空值\n",
    "df.dropna(how='all', subset=['sex']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50d2f4",
   "metadata": {},
   "source": [
    "### 新增列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191737ef",
   "metadata": {},
   "source": [
    "#### append"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986eaf4",
   "metadata": {},
   "source": [
    "每次添加一个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71bf5487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:49:58.363448Z",
     "start_time": "2024-07-03T02:49:58.346022Z"
    }
   },
   "outputs": [],
   "source": [
    "expressions = [] # 创建一个空列表用以存储表达式\n",
    "res = F.col('age') - F.col('score') # 计算表达式\n",
    "expressions.append(res.alias('res1')) # 修改列名并添加到数组中\n",
    "res = res + 100 # 表达式运算\n",
    "expressions.append(res.alias('res2')) # 修改列名并添加到数组中\n",
    "df_resi = df.select(*expressions) # 获取数组结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49843b59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:43:41.030870Z",
     "start_time": "2024-07-03T02:43:22.400504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| res1|res2|\n",
      "+-----+----+\n",
      "|-60.0|40.0|\n",
      "|-62.0|38.0|\n",
      "|-59.0|41.0|\n",
      "|-45.0|55.0|\n",
      "|-41.0|59.0|\n",
      "| -6.0|94.0|\n",
      "|-60.0|40.0|\n",
      "|-62.0|38.0|\n",
      "|-59.0|41.0|\n",
      "| null|null|\n",
      "|-41.0|59.0|\n",
      "| -6.0|94.0|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_resi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0589b9eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:50:20.196233Z",
     "start_time": "2024-07-03T02:50:00.908388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+----+-----+----+\n",
      "| res1|res2|  name| age|score| sex|\n",
      "+-----+----+------+----+-----+----+\n",
      "|-60.0|40.0|   Sam|  28| 88.0|   M|\n",
      "|-62.0|38.0| Flora|  28| 90.0|   F|\n",
      "|-59.0|41.0|   Run|   1| 60.0|null|\n",
      "|-45.0|55.0| Peter|  55|100.0|   M|\n",
      "|-41.0|59.0|   Mei|  54| 95.0|   F|\n",
      "| -6.0|94.0|   Lei|  59| 65.0|   F|\n",
      "|-60.0|40.0|  Sam1|  28| 88.0|   M|\n",
      "|-62.0|38.0|Flora1|  28| 90.0|   F|\n",
      "|-59.0|41.0|  Run1|   1| 60.0|null|\n",
      "| null|null|Peter1|null| null|   M|\n",
      "|-41.0|59.0|  Mei1|  54| 95.0|   F|\n",
      "| -6.0|94.0|  Lei1|  59| 65.0|   F|\n",
      "+-----+----+------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expressions.extend(df.columns) # 原始列\n",
    "df_resi = df.select(*expressions) # 获取数组结果\n",
    "df_resi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf8d01",
   "metadata": {},
   "source": [
    "#### extend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4657ab2",
   "metadata": {},
   "source": [
    "每次可添加一个数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01f7f6fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:18:08.705429Z",
     "start_time": "2024-07-03T02:18:08.677318Z"
    }
   },
   "outputs": [],
   "source": [
    "expressions = [] # 创建一个空列表用以存储表达式\n",
    "res1 = F.col('age') - F.col('score') # 计算表达式1\n",
    "res2 = F.col('score') - F.col('age') # 计算表达式2\n",
    "expressions.extend(df.columns) # 原始列\n",
    "expressions.extend([res1.alias('res1'), res2.alias('res2')]) # 修改列名并添加到数组中\n",
    "df_resi = df.select(*expressions) # 获取数组结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef0a05cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:18:27.558921Z",
     "start_time": "2024-07-03T02:18:12.195826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+----+-----+----+\n",
      "|  name| age|score| sex| res1|res2|\n",
      "+------+----+-----+----+-----+----+\n",
      "|   Sam|  28| 88.0|   M|-60.0|60.0|\n",
      "| Flora|  28| 90.0|   F|-62.0|62.0|\n",
      "|   Run|   1| 60.0|null|-59.0|59.0|\n",
      "| Peter|  55|100.0|   M|-45.0|45.0|\n",
      "|   Mei|  54| 95.0|   F|-41.0|41.0|\n",
      "|   Lei|  59| 65.0|   F| -6.0| 6.0|\n",
      "|  Sam1|  28| 88.0|   M|-60.0|60.0|\n",
      "|Flora1|  28| 90.0|   F|-62.0|62.0|\n",
      "|  Run1|   1| 60.0|null|-59.0|59.0|\n",
      "|Peter1|null| null|   M| null|null|\n",
      "|  Mei1|  54| 95.0|   F|-41.0|41.0|\n",
      "|  Lei1|  59| 65.0|   F| -6.0| 6.0|\n",
      "+------+----+-----+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_resi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd407ea",
   "metadata": {},
   "source": [
    "#### lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "108e1125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T02:22:32.060979Z",
     "start_time": "2024-03-20T02:22:16.148234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-------+\n",
      "| name|age|score| sex|country|\n",
      "+-----+---+-----+----+-------+\n",
      "|  Sam| 28|   88|   M|  China|\n",
      "|Flora| 28|   90|   F|  China|\n",
      "|  Run|  1|   60|null|  China|\n",
      "|Peter| 55|  100|   M|  China|\n",
      "|  Mei| 54|   95|   F|  China|\n",
      "+-----+---+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "# 添加常量列，值都是相同的\n",
    "df.withColumn(\"country\", lit(\"China\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1805448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T02:32:16.543564Z",
     "start_time": "2024-03-20T02:31:44.604426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+---+\n",
      "| name|age|score| sex|sum|\n",
      "+-----+---+-----+----+---+\n",
      "|  Sam| 28|   88|   M|433|\n",
      "|Flora| 28|   90|   F|433|\n",
      "|  Run|  1|   60|null|433|\n",
      "|Peter| 55|  100|   M|433|\n",
      "|  Mei| 54|   95|   F|433|\n",
      "+-----+---+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 添加统计列\n",
    "df.withColumn(\"sum\", lit(df.agg(F.sum('score')).collect()[0][0])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dafbf8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T02:26:29.968487Z",
     "start_time": "2024-03-20T02:26:14.528248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-------+\n",
      "| name|age|score| sex|new_age|\n",
      "+-----+---+-----+----+-------+\n",
      "|  Sam| 28|   88|   M|     33|\n",
      "|Flora| 28|   90|   F|     33|\n",
      "|  Run|  1|   60|null|      6|\n",
      "|Peter| 55|  100|   M|     60|\n",
      "|  Mei| 54|   95|   F|     59|\n",
      "+-----+---+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 增加年龄列的值\n",
    "df.withColumn(\"new_age\", F.col(\"age\") + lit(5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75f96a",
   "metadata": {},
   "source": [
    "##### 列值替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc081f15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T03:33:34.077828Z",
     "start_time": "2024-03-19T03:33:18.000706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-------+\n",
      "| name|age|score| sex|country|\n",
      "+-----+---+-----+----+-------+\n",
      "|  Sam| 28|   88|   M|    USA|\n",
      "|Flora| 28|   90|   F|    USA|\n",
      "|  Run|  1|   60|null|    USA|\n",
      "|Peter| 55|  100|   M|    USA|\n",
      "|  Mei| 54|   95|   F|    USA|\n",
      "+-----+---+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 列替换值\n",
    "df.withColumn(\"country\", lit(\"USA\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc095ba",
   "metadata": {},
   "source": [
    "#### withColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8efc8",
   "metadata": {},
   "source": [
    "**尽量不要用withColumn，会导致内存变大，运行速度变慢，可以用append代替。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8f30660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:52:47.942012Z",
     "start_time": "2023-10-30T01:52:32.952337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+----------+\n",
      "| name|age|score| sex|birth_year|\n",
      "+-----+---+-----+----+----------+\n",
      "|  Sam| 28|   88|   M|      1993|\n",
      "|Flora| 28|   90|   F|      1993|\n",
      "|  Run|  1|   60|null|      2020|\n",
      "|Peter| 55|  100|   M|      1966|\n",
      "|  Mei| 54|   95|   F|      1967|\n",
      "+-----+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 新增相关列，新增的列和之前列有关\n",
    "df1 = df.withColumn(\"birth_year\", 2021 - df.age)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7944fc2",
   "metadata": {},
   "source": [
    "#### monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f072c285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T09:18:32.828138Z",
     "start_time": "2023-11-10T09:18:18.566801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|country|\n",
      "+-------+\n",
      "|  China|\n",
      "|    USA|\n",
      "|  China|\n",
      "|     UK|\n",
      "|  India|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建另一个SparkDataFrame\n",
    "list_values = [[\"China\"],[\"USA\"],[\"China\"],[\"UK\"],[\"India\"]]\n",
    "df1 = spark.createDataFrame(list_values, [\"country\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4563c37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:16:52.160808Z",
     "start_time": "2023-11-09T10:16:23.496011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-------+\n",
      "| name|age|score| sex|country|\n",
      "+-----+---+-----+----+-------+\n",
      "|  Sam| 28|   88|   M|  China|\n",
      "|Flora| 28|   90|   F|    USA|\n",
      "|  Run|  1|   60|null|  China|\n",
      "|Peter| 55|  100|   M|     UK|\n",
      "|  Mei| 54|   95|   F|  India|\n",
      "+-----+---+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#新增无关列，新增的列和之前列无关\n",
    "#先设置自增长列，根据自增长列的序号join，最后匹配，自增长列和分块有关，不同块值不同，并不能完全自增长\n",
    "temp = df.withColumn('Row_ID', monotonically_increasing_id())\n",
    "temp1 = df1.withColumn('Row_ID', monotonically_increasing_id())\n",
    "temp2 = temp.join(temp1, temp['Row_ID'] == temp1['Row_ID'], 'left').drop('Row_ID')\n",
    "temp2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c4a11",
   "metadata": {},
   "source": [
    "#### zipWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09af3ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T09:25:02.483746Z",
     "start_time": "2023-11-10T09:22:43.529746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-------+\n",
      "| name|age|score| sex|country|\n",
      "+-----+---+-----+----+-------+\n",
      "|  Sam| 28|   88|   M|  China|\n",
      "|Flora| 28|   90|   F|    USA|\n",
      "|  Run|  1|   60|null|  China|\n",
      "|Peter| 55|  100|   M|     UK|\n",
      "|  Mei| 54|   95|   F|  India|\n",
      "+-----+---+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用`zipWithIndex`方法和`toDF`函数添加递增列\n",
    "temp = df.rdd.zipWithIndex().map(lambda x: x[0] + (x[1],)).toDF(df.columns + ['Row_ID'])\n",
    "temp1 = df1.rdd.zipWithIndex().map(lambda x: x[0] + (x[1],)).toDF(df1.columns + ['Row_ID'])\n",
    "temp2 = temp.join(temp1, on=['Row_ID'], how='left').drop('Row_ID')\n",
    "temp2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32859a5",
   "metadata": {},
   "source": [
    "### 合并列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b272ff9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:21:47.063403Z",
     "start_time": "2023-11-10T01:21:32.905813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+---------+\n",
      "| name|age|score| sex|age_score|\n",
      "+-----+---+-----+----+---------+\n",
      "|  Sam| 28|   88|   M|    28_88|\n",
      "|Flora| 28|   90|   F|    28_90|\n",
      "|  Run|  1|   60|null|     1_60|\n",
      "|Peter| 55|  100|   M|   55_100|\n",
      "|  Mei| 54|   95|   F|    54_95|\n",
      "+-----+---+-----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"age_score\", F.concat_ws('_', F.col(\"age\"), F.col(\"score\")))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6a1612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:15:11.209580Z",
     "start_time": "2023-11-10T01:14:56.814382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+---------+\n",
      "| name|age|score| sex|age_score|\n",
      "+-----+---+-----+----+---------+\n",
      "|  Sam| 28|   88|   M|     2888|\n",
      "|Flora| 28|   90|   F|     2890|\n",
      "|  Run|  1|   60|null|      160|\n",
      "|Peter| 55|  100|   M|    55100|\n",
      "|  Mei| 54|   95|   F|     5495|\n",
      "+-----+---+-----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"age_score\",F.concat(\"age\",\"score\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37321f6",
   "metadata": {},
   "source": [
    "### 重命名列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6afbad57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:53:31.445520Z",
     "start_time": "2023-10-30T01:53:16.775144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+------+\n",
      "| name|age|score|gender|\n",
      "+-----+---+-----+------+\n",
      "|  Sam| 28|   88|     M|\n",
      "|Flora| 28|   90|     F|\n",
      "|  Run|  1|   60|  null|\n",
      "|Peter| 55|  100|     M|\n",
      "|  Mei| 54|   95|     F|\n",
      "+-----+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 重命名列名\n",
    "df1 = df.withColumnRenamed(\"sex\", \"gender\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取别名\n",
    "df['age'].alias('age_value')\n",
    "df.select(df['age'].alias('age_value'),'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43a2f9",
   "metadata": {},
   "source": [
    "### 修改列类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "916e3ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:48:32.854051Z",
     "start_time": "2023-11-06T08:48:18.838117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"age\", df[\"age\"].cast('string'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "099059db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:15:48.598169Z",
     "start_time": "2023-11-06T09:15:07.299673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"age\", df[\"age\"].astype('string'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749ebfe",
   "metadata": {},
   "source": [
    "### 填充列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35cf18cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:59:04.982317Z",
     "start_time": "2023-10-30T01:58:35.463584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|  C1|  C2|\n",
      "+----+----+\n",
      "|   a|null|\n",
      "|   a|   1|\n",
      "|null|   3|\n",
      "|   c|   4|\n",
      "+----+----+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 99|\n",
      "|  a|  1|\n",
      "|  d|  3|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 空值填充操作\n",
    "df1 = spark.createDataFrame(\n",
    "        [(\"a\", None), (\"a\", 1), (None,  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
    "# df2 = df1.na.fill({\"C1\": \"d\", \"C2\": 99})\n",
    "df2 = df1.fillna({\"C1\": \"d\", \"C2\": 99})\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e653b",
   "metadata": {},
   "source": [
    "### 条件过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12a57a",
   "metadata": {},
   "source": [
    "#### first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf69d12e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:22:03.653666Z",
     "start_time": "2024-07-26T09:21:47.195961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(F.first(F.col(\"age\"))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c01e8874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:33:25.073481Z",
     "start_time": "2024-07-26T09:33:20.020260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Sam', age=28, score=88.0, sex='M')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd89f9",
   "metadata": {},
   "source": [
    "#### last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63794940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:22:23.478706Z",
     "start_time": "2024-07-26T09:22:07.303079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(F.last(F.col(\"age\"))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16b2fc",
   "metadata": {},
   "source": [
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8e9864d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:20:32.784980Z",
     "start_time": "2023-11-06T09:19:51.601585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据条件过滤\n",
    "df.filter(df.age.between(50, 90)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5db710ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:34:48.682750Z",
     "start_time": "2023-11-06T09:34:06.445305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+---+\n",
      "|name|age|score|sex|\n",
      "+----+---+-----+---+\n",
      "+----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 可以使用正则的匹配\n",
    "df.filter(df.name.rlike('Me$')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f52507fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:31:52.401152Z",
     "start_time": "2023-11-06T09:31:11.117303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+---+\n",
      "|name|age|score|sex|\n",
      "+----+---+-----+---+\n",
      "| Mei| 54|   95|  F|\n",
      "+----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 返回含有关键词的行\n",
    "df.filter(df.name.like('Mei')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68d09974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:25:48.821779Z",
     "start_time": "2023-11-06T09:25:07.244196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+----+\n",
      "|name|age|score| sex|\n",
      "+----+---+-----+----+\n",
      "| Run|  1|   60|null|\n",
      "+----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 筛选非空的行\n",
    "df.filter(df.sex.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3134270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:24:43.433972Z",
     "start_time": "2023-11-06T09:24:02.127640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|  Sam| 28|   88|  M|\n",
      "|Flora| 28|   90|  F|\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 筛选非空的行\n",
    "df.filter(df.sex.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0d38a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:21:58.451409Z",
     "start_time": "2023-11-06T09:21:17.091493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Peter| 55|  100|  M|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 是否包含某个关键词\n",
    "df.filter(df.age.contains(55)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b1654c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:19:02.095606Z",
     "start_time": "2023-11-06T09:18:21.041188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#单条件过滤\n",
    "df.filter(df.age>50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c30522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#多条件过滤\n",
    "df.filter((df.age>50) | (df.age<10)).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3c21f23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T02:00:00.949470Z",
     "start_time": "2023-10-30T01:59:45.097997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+----+\n",
      "|name|age|score| sex|\n",
      "+----+---+-----+----+\n",
      "| Run|  1|   60|null|\n",
      "+----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"age<18\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279b2d9",
   "metadata": {},
   "source": [
    "#### take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90c0915e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T02:54:58.110406Z",
     "start_time": "2024-03-21T02:54:40.967436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Sam', age=28, score=88, sex='M'),\n",
       " Row(name='Flora', age=28, score=90, sex='F')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取前两行\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abecbb",
   "metadata": {},
   "source": [
    "#### tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cd7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取后两行\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3380dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ps.DataFrame(df.tail(n),columns=df.columns).to_spark() # 最后几行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04805cbe",
   "metadata": {},
   "source": [
    "#### where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ad3b85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T02:18:47.772634Z",
     "start_time": "2024-03-20T02:18:31.943825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|  Sam| 28|   88|  M|\n",
      "|Flora| 28|   90|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#条件选择\n",
    "df.where(df.age==28).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5653168d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T02:39:43.524219Z",
     "start_time": "2024-03-20T02:39:27.422093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|  Sam| 28|   88|  M|\n",
      "|Flora| 28|   90|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 多条件选择，并且只能用&，|符号\n",
    "df.where((df.age==28)|(df.score==88)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65622e09",
   "metadata": {},
   "source": [
    "#### when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "177a7026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:34:01.786458Z",
     "start_time": "2023-11-17T10:33:45.588682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "| name|age|判断|\n",
      "+-----+---+----+\n",
      "|  Sam| 28|   1|\n",
      "|Flora| 28|   1|\n",
      "|  Run|  1|  -1|\n",
      "|Peter| 55|   1|\n",
      "|  Mei| 54|   1|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#when(condition, value1).otherwise(value2)联合使用：\n",
    "#当满足条件condition的指赋值为values1,不满足条件的则赋值为values2.\n",
    "#otherwise表示，不满足条件的情况下，应该赋值为啥\n",
    "df.select(df.name, df.age, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0).alias(\"判断\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ec73fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T02:38:34.958493Z",
     "start_time": "2024-03-20T02:38:17.983387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "| name|age|判断|\n",
      "+-----+---+----+\n",
      "|  Sam| 28|   0|\n",
      "|Flora| 28|   0|\n",
      "|  Run|  1|  -1|\n",
      "|Peter| 55|   0|\n",
      "|  Mei| 54|   0|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#多条件需要用括号括起来，并且只能用&，|符号\n",
    "df.select(df.name, df.age, F.when((df.age > 4)&(df.age < 2), 1).when(df.age < 3, -1).otherwise(0).alias(\"判断\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f653c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"value_th\", F.when(df[\"age\"] >= 10, 1).when(df[\"value\"] <= 20, -1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4454d",
   "metadata": {},
   "source": [
    "#### isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2820dd66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:46:31.242756Z",
     "start_time": "2023-11-10T01:46:17.041901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|  Sam| 28|   88|  M|\n",
      "|Flora| 28|   90|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 列表参数\n",
    "filter_list = [\"Sam\", \"Flora\"]\n",
    "\n",
    "# 使用isin函数进行筛选\n",
    "filtered_df = df.filter(F.col(\"name\").isin(filter_list))\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43ca6cd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:28:22.815863Z",
     "start_time": "2023-11-06T09:27:41.729097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 返回包含某些值的行\n",
    "df.filter(df.name.isin('Peter', 'Mei')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4b198",
   "metadata": {},
   "source": [
    "#### isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2515e15e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T02:19:33.495203Z",
     "start_time": "2024-03-21T02:19:14.243283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|(name IS NULL)|\n",
      "+--------------+\n",
      "|         false|\n",
      "|         false|\n",
      "|         false|\n",
      "|         false|\n",
      "|         false|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark中DataFrame的缺失值是None类型的，可用F.isnull()来判定。\n",
    "df.select(F.isnull(df['name'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1c43b4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T02:20:02.592686Z",
     "start_time": "2024-03-21T02:19:42.356311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+----+\n",
      "|name|age|score| sex|\n",
      "+----+---+-----+----+\n",
      "| Run|  1|   60|null|\n",
      "+----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.filter(F.isnull(\"sex\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed051ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:50:20.468006Z",
     "start_time": "2023-11-10T01:50:20.463271Z"
    }
   },
   "source": [
    "#### coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b41a04ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:49:42.854688Z",
     "start_time": "2023-11-10T01:49:28.574119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+--------+\n",
      "| name|age|score| sex|age_name|\n",
      "+-----+---+-----+----+--------+\n",
      "|  Sam| 28|   88|   M|      28|\n",
      "|Flora| 28|   90|   F|      28|\n",
      "|  Run|  1|   60|null|       1|\n",
      "|Peter| 55|  100|   M|      55|\n",
      "|  Mei| 54|   95|   F|      54|\n",
      "+-----+---+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#返回第一个不为null的列\n",
    "df.withColumn('age_name',F.coalesce(df.age,df.name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a49ea",
   "metadata": {},
   "source": [
    "#### expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80768de6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:53:46.106983Z",
     "start_time": "2023-11-10T01:53:31.718253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|upper(name)|\n",
      "+-----------+\n",
      "|        SAM|\n",
      "|      FLORA|\n",
      "|        RUN|\n",
      "|      PETER|\n",
      "|        MEI|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#expr()方法解析给定的SQL表达式\n",
    "df.select(F.expr('upper(name)')).show() #字母大写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9415a268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:54:29.101986Z",
     "start_time": "2023-11-10T01:54:14.750603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|upper(name)|\n",
      "+-----------+\n",
      "|        SAM|\n",
      "|      FLORA|\n",
      "|        RUN|\n",
      "|      PETER|\n",
      "|        MEI|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#expr(~) 方法通常可以使用 PySpark DataFrame 的 selectExpr(~) 方法编写得更简洁\n",
    "df.selectExpr('upper(name)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c661a689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:55:25.620995Z",
     "start_time": "2023-11-10T01:55:11.228947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|result|\n",
      "+------+\n",
      "|  true|\n",
      "| false|\n",
      "| false|\n",
      "| false|\n",
      "| false|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#使用expr方法解析复杂的SQL表达式\n",
    "df.select(F.expr('age > 20 AND name LIKE \"S%\"').alias('result')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "380fd3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:57:39.523083Z",
     "start_time": "2023-11-10T01:57:24.811853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+------+\n",
      "| name|age|score| sex|status|\n",
      "+-----+---+-----+----+------+\n",
      "|  Sam| 28|   88|   M|     1|\n",
      "|Flora| 28|   90|   F|     1|\n",
      "|  Run|  1|   60|null|     1|\n",
      "|Peter| 55|  100|   M|     0|\n",
      "|  Mei| 54|   95|   F|     0|\n",
      "+-----+---+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col = F.expr('CASE WHEN age < 40 THEN \"1\" ELSE \"0\" END').alias('result')\n",
    "df.withColumn('status', col).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a4f26",
   "metadata": {},
   "source": [
    "#### substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f154cae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:58:59.585436Z",
     "start_time": "2023-11-10T01:58:45.491698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----+\n",
      "| name|age|score| sex|age_1|\n",
      "+-----+---+-----+----+-----+\n",
      "|  Sam| 28|   88|   M|   am|\n",
      "|Flora| 28|   90|   F|  lor|\n",
      "|  Run|  1|   60|null|   un|\n",
      "|Peter| 55|  100|   M|  ete|\n",
      "|  Mei| 54|   95|   F|   ei|\n",
      "+-----+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"age_1\", F.substring(df.name, 2, 3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c498866",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83158639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:34:51.528968Z",
     "start_time": "2023-11-08T07:34:22.449290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+\n",
      "|df1_id|df1_num|df2_num|\n",
      "+------+-------+-------+\n",
      "|     d|      1|   null|\n",
      "|     c|      4|   null|\n",
      "|     b|      3|      3|\n",
      "|     a|      1|      1|\n",
      "+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(\"a\", 1), (\"d\", 1), (\"b\", 3), (\"c\", 4)],\n",
    "                            [\"id\", \"num1\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"id\", \"num2\"])\n",
    "df1.join(df2, df1.id == df2.id, 'left').select(df1.id.alias(\"df1_id\"),\n",
    "                                               df1.num1.alias(\"df1_num\"),\n",
    "                                               df2.num2.alias(\"df2_num\")\n",
    "                                               ).sort([\"df1_id\"], ascending=False)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe73e5",
   "metadata": {},
   "source": [
    "### 聚合函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac313e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:31:30.903052Z",
     "start_time": "2023-11-06T08:31:30.900168Z"
    }
   },
   "source": [
    "- avg(\\*cols)     ——   计算每组中一列或多列的平均值\n",
    "- count()          ——   计算每组中一共有多少行，返回DataFrame有2列，一列为分组的组名，另一列为行总数\n",
    "- max(\\*cols)    ——   计算每组中一列或多列的最大值\n",
    "- mean(\\*cols)  ——  计算每组中一列或多列的平均值\n",
    "- min(\\*cols)     ——  计算每组中一列或多列的最小值\n",
    "- sum(\\*cols)    ——   计算每组中一列或多列的总和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ace20a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T03:19:13.402984Z",
     "start_time": "2023-12-14T03:18:57.168571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|最小年龄|平均年龄|\n",
      "+--------+--------+\n",
      "|       1|    33.2|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据某一列进行聚合\n",
    "df.agg(\n",
    "    F.min(df.age).alias(\"最小年龄\"),\n",
    "    F.expr(\"avg(age)\").alias(\"平均年龄\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ed4f7",
   "metadata": {},
   "source": [
    "### 分组聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da050ec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T03:16:32.542941Z",
     "start_time": "2023-12-14T03:16:16.029619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+------------+\n",
      "| sex|最小年龄|平均年龄|    姓名集合|\n",
      "+----+--------+--------+------------+\n",
      "|   M|      28|    41.5|[Sam, Peter]|\n",
      "|   F|      28|    41.0|[Flora, Mei]|\n",
      "|null|       1|     1.0|       [Run]|\n",
      "+----+--------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据某几列进行聚合，如有多列用列表写在一起，如 df.groupBy([\"sex\", \"age\"])\n",
    "df.groupBy(\"sex\").agg(F.min(df.age).alias(\"最小年龄\"),\n",
    "                      F.expr(\"avg(age)\").alias(\"平均年龄\"),\n",
    "                      F.expr(\"collect_list(name)\").alias(\"姓名集合\")\n",
    "                      ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "350fcc80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:32:29.443238Z",
     "start_time": "2023-11-06T08:32:01.671632Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对每一行进行函数方法的应用\n",
    "def f(person):\n",
    "    print(person.name)\n",
    "df.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ebef2d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:32:45.988727Z",
     "start_time": "2023-11-06T08:32:32.131560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+------+\n",
      "| name|age|score|   sex|\n",
      "+-----+---+-----+------+\n",
      "|  Sam| 28|   88|  Male|\n",
      "|Flora| 28|   90|Female|\n",
      "|  Run|  1|   60|  null|\n",
      "|Peter| 55|  100|  Male|\n",
      "|  Mei| 54|   95|Female|\n",
      "+-----+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 修改df里的某些值\n",
    "df1 = df.na.replace({\"M\": \"Male\", \"F\": \"Female\"})\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201e05e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T03:19:59.954282Z",
     "start_time": "2023-12-14T03:19:43.925633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算中位数\n",
    "df.approxQuantile(\"age\", [0.5], 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b8b47",
   "metadata": {},
   "source": [
    "### union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16f2fb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T01:59:52.904327Z",
     "start_time": "2024-05-06T01:59:05.289857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|num|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  d|  1|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| id|num|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  d|  1|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 相当于SQL里的union all操作\n",
    "df1 = spark.createDataFrame(\n",
    "        [(\"a\", 1), (\"d\", 1), (\"b\",  3), (\"c\", 4)], [\"id\", \"num\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"id\", \"num\"])\n",
    "df1.union(df2).show()\n",
    "df1.unionAll(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755d3e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:11:13.034071Z",
     "start_time": "2024-05-06T02:10:41.870261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col0|col1|col2|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   6|   4|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据列名来进行合并数据集\n",
    "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
    "df1.unionByName(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04e932",
   "metadata": {},
   "source": [
    "### 集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "089437a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:39:14.891471Z",
     "start_time": "2023-11-06T08:38:47.492669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|sentence|\n",
      "+-----+--------+\n",
      "|    1|     asf|\n",
      "|    2|    2143|\n",
      "|    3|    rfds|\n",
      "+-----+--------+\n",
      "\n",
      "+-----+--------+\n",
      "|label|sentence|\n",
      "+-----+--------+\n",
      "|    1|     asf|\n",
      "|    2|    2143|\n",
      "|    4|  f8934y|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame = spark.createDataFrame(\n",
    "    ((1, \"asf\"), (2, \"2143\"), (3, \"rfds\"))).toDF(\"label\", \"sentence\")\n",
    "sentenceDataFrame.show()\n",
    "\n",
    "sentenceDataFrame1 = spark.createDataFrame(\n",
    "    ((1, \"asf\"), (2, \"2143\"), (4, \"f8934y\"))).toDF(\"label\", \"sentence\")\n",
    "sentenceDataFrame1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "486a6416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:38:31.695533Z",
     "start_time": "2023-11-06T08:38:03.895383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sentence|\n",
      "+--------+\n",
      "|  f8934y|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 差集\n",
    "newDF = sentenceDataFrame1.select(\"sentence\").subtract(sentenceDataFrame.select(\"sentence\"))\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e26990b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:39:51.585082Z",
     "start_time": "2023-11-06T08:39:23.475982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sentence|\n",
      "+--------+\n",
      "|     asf|\n",
      "|    2143|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 交集\n",
    "newDF = sentenceDataFrame1.select(\"sentence\").intersect(sentenceDataFrame.select(\"sentence\"))\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50302746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:40:22.622395Z",
     "start_time": "2023-11-06T08:39:55.167921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sentence|\n",
      "+--------+\n",
      "|     asf|\n",
      "|    2143|\n",
      "|  f8934y|\n",
      "|     asf|\n",
      "|    2143|\n",
      "|    rfds|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 并集\n",
    "newDF = sentenceDataFrame1.select(\"sentence\").union(sentenceDataFrame.select(\"sentence\"))\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6c0b4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:40:56.574051Z",
     "start_time": "2023-11-06T08:40:28.795023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sentence|\n",
      "+--------+\n",
      "|     asf|\n",
      "|    2143|\n",
      "|  f8934y|\n",
      "|    rfds|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 并集 + 去重\n",
    "newDF = sentenceDataFrame1.select(\"sentence\").union(sentenceDataFrame.select(\"sentence\")).distinct()\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ac520",
   "metadata": {},
   "source": [
    "### 列名替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.pandas_api()\n",
    "data.columns = data.columns.str.replace(\".\", \"#\")\n",
    "data = data.to_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b994db",
   "metadata": {},
   "source": [
    "## 自定义函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892d414",
   "metadata": {},
   "source": [
    "- 在spark 1.x 版本的时候，利用pyspark包中的 udf() 来开发用户自定义函数，自定义的函数只能接收单一数值，因此当你向自定义函数中传递 dataframe的一个列时，自定义函数内部的处理方式就是执行了for循环，将传入列中的每个值逐一处理，这样效率比较低，在加上scala进程与python进程间数据传递、序列化与反序列化的时间消耗，用户自定义函数的执行效率比较低。**单点类型**\n",
    "- 在spark 2.x 版本中，以上的问题有了很大改善，新增了用户自定义函数的定义接口pandas_udf，并且对scala进程与python进程间的数据传递过程做了优化，数据从scala进程到python进程时，需要进行序列化，而当python进程执行完毕，将数据传递回scala进行时，则无需进行反序列化，这是因为使用了Apache Arrow 这种基于内存的列式数据存储格式，这样就节省了时间，当然需要将spark参数spark.sql.execution.arrow.enabled设置为true。总的来说，目前在实际开发中，用到的基本都是pandas_udf了。**pandas.Series类型**\n",
    "\n",
    "**用户自定义的python函数是会提交到python进程中执行的，因此函数中使用的应该是python和各个包(pyspark之外)的函数与数对象，而不是pyspark中的函数与数据对象。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550097e6",
   "metadata": {},
   "source": [
    "### udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b53c2",
   "metadata": {},
   "source": [
    "#### 匿名函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bec12f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T08:32:24.791960Z",
     "start_time": "2023-11-17T08:31:53.326584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----+\n",
      "| name|age|score| sex|age+1|\n",
      "+-----+---+-----+----+-----+\n",
      "|  Sam| 28|   88|   M|   29|\n",
      "|Flora| 28|   90|   F|   29|\n",
      "|  Run|  1|   60|null|    2|\n",
      "|Peter| 55|  100|   M|   56|\n",
      "|  Mei| 54|   95|   F|   55|\n",
      "+-----+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "func = F.udf(lambda x: x+1)\n",
    "df1 = df.withColumn('age+1',func(df[\"age\"]))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ce8c9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T08:54:28.516359Z",
     "start_time": "2023-11-17T08:53:57.230095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----+\n",
      "| name|age|score| sex|age+1|\n",
      "+-----+---+-----+----+-----+\n",
      "|  Sam| 28|   88|   M|   29|\n",
      "|Flora| 28|   90|   F|   29|\n",
      "|  Run|  1|   60|null|    2|\n",
      "|Peter| 55|  100|   M|   56|\n",
      "|  Mei| 54|   95|   F|   55|\n",
      "+-----+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#Use udf to define a row-at-a-time udf\n",
    "@udf()\n",
    "# Input/output are both a single double value\n",
    "def plus_one(x):\n",
    "    return x + 1\n",
    "df1 = df.withColumn('age+1',plus_one(df.age))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ce655",
   "metadata": {},
   "source": [
    "#### 普通函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f5ba0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T08:36:46.917980Z",
     "start_time": "2023-11-17T08:36:15.222872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+------+\n",
      "| name|age|score| sex|degree|\n",
      "+-----+---+-----+----+------+\n",
      "|  Sam| 28|   88|   M|     B|\n",
      "|Flora| 28|   90|   F|     A|\n",
      "|  Run|  1|   60|null|     C|\n",
      "|Peter| 55|  100|   M|     A|\n",
      "|  Mei| 54|   95|   F|     A|\n",
      "+-----+---+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数来计算平方\n",
    "def score(x):\n",
    "    if x > 80 and x < 90:\n",
    "        return 'B'\n",
    "    elif x >= 90:\n",
    "        return 'A'\n",
    "    else:\n",
    "        return 'C'\n",
    "\n",
    "# 注册UDF\n",
    "score_udf = F.udf(score)\n",
    "\n",
    "# 使用UDF对DataFrame进行操作\n",
    "df1 = df.withColumn('degree', score_udf(df['score']))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39003517",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T08:55:36.882427Z",
     "start_time": "2023-11-17T08:55:05.381213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+------+\n",
      "| name|age|score| sex|degree|\n",
      "+-----+---+-----+----+------+\n",
      "|  Sam| 28|   88|   M|     B|\n",
      "|Flora| 28|   90|   F|     A|\n",
      "|  Run|  1|   60|null|     C|\n",
      "|Peter| 55|  100|   M|     A|\n",
      "|  Mei| 54|   95|   F|     A|\n",
      "+-----+---+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf()\n",
    "# 定义一个函数来计算平方\n",
    "def score(x):\n",
    "    if x > 80 and x < 90:\n",
    "        return 'B'\n",
    "    elif x >= 90:\n",
    "        return 'A'\n",
    "    else:\n",
    "        return 'C'\n",
    "\n",
    "# 使用UDF对DataFrame进行操作\n",
    "df1 = df.withColumn('degree', score(df['score']))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4b181",
   "metadata": {},
   "source": [
    "### pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d57e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T06:10:57.525821Z",
     "start_time": "2024-06-13T06:10:57.521962Z"
    },
    "execution": {
     "iopub.execute_input": "2024-06-13T11:48:40.422123Z",
     "iopub.status.busy": "2024-06-13T11:48:40.421455Z",
     "iopub.status.idle": "2024-06-13T11:48:40.432555Z",
     "shell.execute_reply": "2024-06-13T11:48:40.431117Z",
     "shell.execute_reply.started": "2024-06-13T11:48:40.422123Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e783f4",
   "metadata": {},
   "source": [
    "pandas_udf函数支持定义三种类型的函数，分别为SCALAR、GROUP_MAP、GROUP_AGG，在pyspark.sql.function.PandasUDFType 中做了定义，默认SCALAR类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef9d0b",
   "metadata": {},
   "source": [
    "#### SCALAR 类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f3851",
   "metadata": {},
   "source": [
    "SCALAR 类型自定义函数可以**接收多个列的输入，也只能接受列输入，但永远只会输出一个列**，并且返回的数据类型是pyspark.sql.types 中定义的数据类型。应用SCALAR类型自定义函数时，在自定义函数内部应当执行的一些逻辑不那么复杂的操作，如果不是这样，你可以使用其它的方式来满足需求。例如当你需要对每一条数据用不同的、比较复杂的逻辑来处理，那可以用 GROUP_MAP 类型来自定义函数，或者将dataframe转换为rdd，然后用mapValue函数来处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161f6af",
   "metadata": {},
   "source": [
    "该类型支持用户自定义向量型操作的函数，也就是pandas、numpy中的向量操作，传入的格式为pd.Series，传出的格式为pd.Series。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c1501",
   "metadata": {},
   "source": [
    "使用scipy去计算一个值的累计正太分布概率值，stats.norm.cdf 可以使用在一个标量或者是一个pandas.Series上面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18d10c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T09:36:22.712002Z",
     "start_time": "2023-11-17T09:35:41.776993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+----------------------+\n",
      "| name|age|score| sex|cumulative_probability|\n",
      "+-----+---+-----+----+----------------------+\n",
      "|  Sam| 28|   88|   M|                   1.0|\n",
      "|Flora| 28|   90|   F|                   1.0|\n",
      "|  Run|  1|   60|null|    0.8413447460685429|\n",
      "|Peter| 55|  100|   M|                   1.0|\n",
      "|  Mei| 54|   95|   F|                   1.0|\n",
      "+-----+---+-----+----+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "@pandas_udf(returnType='double')\n",
    "def cdf(v):\n",
    "    return pd.Series(stats.norm.cdf(v))\n",
    "\n",
    "df1 = df.withColumn('cumulative_probability',cdf(df.age))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39e71b",
   "metadata": {},
   "source": [
    "#### GROUP_MAP 类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5756b72",
   "metadata": {},
   "source": [
    "该类型支持用户将dataframe按照一组字段分组，然后对分组后的多个dataframe分别用户自定义函数处理，在用户自定义函数中，处理的对象是pandas 的DataFrame。\n",
    "\n",
    "**该方法在@pandas_udf的输入列和输出列必须一致，类型也必须一致，否则将报错。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a7aeb-745c-497e-b026-62ad9d605171",
   "metadata": {},
   "source": [
    "##### 多参数udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7cc252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T06:20:00.858677Z",
     "start_time": "2024-06-13T06:19:35.994237Z"
    },
    "execution": {
     "iopub.execute_input": "2024-06-13T11:49:06.963435Z",
     "iopub.status.busy": "2024-06-13T11:49:06.962430Z",
     "iopub.status.idle": "2024-06-13T11:49:18.083262Z",
     "shell.execute_reply": "2024-06-13T11:49:18.081990Z",
     "shell.execute_reply.started": "2024-06-13T11:49:06.963435Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pyspark\\sql\\pandas\\group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+----+\n",
      "|  name| age|score| sex|\n",
      "+------+----+-----+----+\n",
      "|   Run|   4| 63.0|NULL|\n",
      "|  Run1|   4| 63.0|NULL|\n",
      "|   Lei|  62| 68.0|   F|\n",
      "|  Lei1|  62| 68.0|   F|\n",
      "|   Sam|  31| 91.0|   M|\n",
      "|  Sam1|  31| 91.0|   M|\n",
      "| Flora|  31| 93.0|   F|\n",
      "|Flora1|  31| 93.0|   F|\n",
      "|   Mei|  57| 98.0|   F|\n",
      "|  Mei1|  57| 98.0|   F|\n",
      "| Peter|  58|103.0|   M|\n",
      "|Peter1|NULL| NULL|   M|\n",
      "+------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas版本的函数，直接传入DataFrame\n",
    "def func(df, column):\n",
    "    df[column] = df[column] + 3\n",
    "    df.sort_values(by=[column], inplace=True, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# 增加一列不变列，用于在分组的时候将数据划分到一起\n",
    "df = df.withColumn('id', F.lit(1))\n",
    "\n",
    "# 依次使'age', 'score'列分别加3\n",
    "for i in ['age', 'score']:\n",
    "    # 装饰函数，注册UDF\n",
    "    @pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)\n",
    "    def func_udf(df):\n",
    "        return func(df, i)\n",
    "    df = df.groupby('id').apply(func_udf)\n",
    "df = df.drop('id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618252e2-aebf-4bf6-81c9-b467858ef296",
   "metadata": {},
   "source": [
    "##### 单参数udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bff5340-1cb5-4901-9fe4-7ba8dbe75de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:07:49.749410Z",
     "iopub.status.busy": "2024-06-13T12:07:49.748402Z",
     "iopub.status.idle": "2024-06-13T12:07:57.981422Z",
     "shell.execute_reply": "2024-06-13T12:07:57.980464Z",
     "shell.execute_reply.started": "2024-06-13T12:07:49.749410Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pyspark\\sql\\pandas\\group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| age|income|\n",
      "+----+------+\n",
      "| 1.0|  NULL|\n",
      "| 2.0|  NULL|\n",
      "| 2.0|  NULL|\n",
      "|NULL|   3.0|\n",
      "| 4.0|   4.0|\n",
      "| 5.0|   5.0|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas版本的函数，直接传入DataFrame\n",
    "def func(df):\n",
    "    return df\n",
    "\n",
    "# 增加一列不变列，用于在分组的时候将数据划分到一起\n",
    "df = df.withColumn('id', F.lit(1))\n",
    "\n",
    "# 装饰函数，注册UDF\n",
    "@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)\n",
    "def func_udf(df):\n",
    "    return func(df)\n",
    "df = df.groupby('id').apply(func_udf)\n",
    "df = df.drop('id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36cbb2",
   "metadata": {},
   "source": [
    "#### GROUP_AGG 类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e629f611",
   "metadata": {},
   "source": [
    "这种类型用得不多，因为可以直接被`dataframe.groupby().agg(sf.min())` 这样的函数取代。它的功能就是分组计算统计值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43aad40f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:18:38.907397Z",
     "start_time": "2023-11-17T10:18:20.344977Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|func(score)|\n",
      "+---+-----------+\n",
      "|  1|         60|\n",
      "| 28|         89|\n",
      "| 54|         95|\n",
      "| 55|        100|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType,IntegerType,StructType,StructField\n",
    "def func(x):\n",
    "    return x.mean()\n",
    "\n",
    "func_udf = pandas_udf(func, IntegerType(), PandasUDFType.GROUPED_AGG)\n",
    "df1 = df.groupby('age').agg(func_udf(df['score']))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd3c20",
   "metadata": {},
   "source": [
    "## 格式转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74900031",
   "metadata": {},
   "source": [
    "### 转pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33866c6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:06:41.850013Z",
     "start_time": "2023-11-06T09:06:27.733240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>score</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sam</td>\n",
       "      <td>28</td>\n",
       "      <td>88</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flora</td>\n",
       "      <td>28</td>\n",
       "      <td>90</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peter</td>\n",
       "      <td>55</td>\n",
       "      <td>100</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mei</td>\n",
       "      <td>54</td>\n",
       "      <td>95</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  age  score   sex\n",
       "0    Sam   28     88     M\n",
       "1  Flora   28     90     F\n",
       "2    Run    1     60  None\n",
       "3  Peter   55    100     M\n",
       "4    Mei   54     95     F"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825486b",
   "metadata": {},
   "source": [
    "转化为pandas，但是该数据要读入内存，如果数据量大的话，很难跑得动\n",
    "\n",
    "两者的异同：\n",
    "\n",
    "- Pyspark DataFrame是在分布式节点上运行一些数据操作，而pandas是不可能的；\n",
    "- Pyspark DataFrame的数据反映比较缓慢，没有Pandas那么及时反映；\n",
    "- Pyspark DataFrame的数据框是不可变的，不能任意添加列，只能通过合并进行；\n",
    "- pandas比Pyspark DataFrame有更多方便的操作以及很强大\n",
    "\n",
    "如果包含时间标签，需要先将时间格式转为字符串  \n",
    "`df = df.withColumn(\"ts\", df[\"ts\"].cast('string'))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faf47fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T01:51:38.962444Z",
     "start_time": "2024-07-30T01:51:21.783361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>score</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sam</td>\n",
       "      <td>28.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flora</td>\n",
       "      <td>28.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peter</td>\n",
       "      <td>55.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mei</td>\n",
       "      <td>54.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lei</td>\n",
       "      <td>59.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sam1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Flora1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Run1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Peter1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mei1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lei1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name   age  score   sex\n",
       "0      Sam  28.0   88.0     M\n",
       "1    Flora  28.0   90.0     F\n",
       "2      Run   1.0   60.0  None\n",
       "3    Peter  55.0  100.0     M\n",
       "4      Mei  54.0   95.0     F\n",
       "5      Lei  59.0   65.0     F\n",
       "6     Sam1  28.0   88.0     M\n",
       "7   Flora1  28.0   90.0     F\n",
       "8     Run1   1.0   60.0  None\n",
       "9   Peter1   NaN    NaN     M\n",
       "10    Mei1  54.0   95.0     F\n",
       "11    Lei1  59.0   65.0     F"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pandas_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd5fd8",
   "metadata": {},
   "source": [
    "### 转spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc14f74a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:11:39.794837Z",
     "start_time": "2023-11-06T09:11:11.949328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\pyspark\\pandas\\utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "df1 = df.toPandas()\n",
    "df = ps.DataFrame(df1).to_spark()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8234a8",
   "metadata": {},
   "source": [
    "### 转rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e867df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:14:56.784578Z",
     "start_time": "2023-11-06T09:14:02.230429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df = df.rdd\n",
    "df = rdd_df.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0722bbc",
   "metadata": {},
   "source": [
    "### 转字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b76790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T06:33:57.223084Z",
     "start_time": "2024-09-23T06:33:41.336125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Sam', 'age': 28, 'score': 88.0, 'sex': 'M'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()[0].asDict() # 把第一行转为字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74925842",
   "metadata": {},
   "source": [
    "## Pandas_UDF与toPandas的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5558650d",
   "metadata": {},
   "source": [
    "@pandas_udf 创建一个向量化的用户定义函数(UDF)，利用了panda的矢量化特性，是udf的一种更快的替代方案，因此适用于分布式数据集。\n",
    "\n",
    "toPandas将分布式spark数据集转换为pandas数据集，对pandas数据集进行本地化，并且所有数据都驻留在驱动程序内存中，因此此方法仅在预期生成的pandas DataFrame较小的情况下使用。\n",
    "\n",
    "换句话说，@pandas_udf使用panda API来处理分布式数据集，而toPandas()将分布式数据集转换为本地数据，然后使用pandas进行处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5deb2",
   "metadata": {},
   "source": [
    "# SparkDataFrame时间处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e21ce",
   "metadata": {},
   "source": [
    "**SparkDataFrame支持两种时间格式，包括：'date'和'timestamp'。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791c7e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T02:17:23.788891Z",
     "start_time": "2024-10-14T02:17:05.995438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      time|\n",
      "+---+----------+\n",
      "|  1|2020-02-03|\n",
      "|  2|2019-03-05|\n",
      "|  3|2021-03-09|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-03\"],[\"2\",\"2019-03-05\"],[\"3\",\"2021-03-09\"]]\n",
    "df=spark.createDataFrame(data, [\"id\",\"time\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a9a2a",
   "metadata": {},
   "source": [
    "## 判断时间类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e2d352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T02:18:36.826368Z",
     "start_time": "2024-10-14T02:18:36.821761Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_time_col(dataframe):\n",
    "    for column, d_type in dataframe.dtypes:\n",
    "        if d_type in ['timestamp', 'date']:\n",
    "            return column\n",
    "    raise RuntimeError(f\"该dataframe不存在时间列！{dataframe.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f1afb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T02:18:49.636688Z",
     "start_time": "2024-10-14T02:18:49.607662Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "该dataframe不存在时间列！[('id', 'string'), ('time', 'string')]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m time_col \u001b[38;5;241m=\u001b[39m is_time_col(df)\n\u001b[0;32m      2\u001b[0m time_col\n",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m, in \u001b[0;36mis_time_col\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m column\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m该dataframe不存在时间列！\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataframe\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 该dataframe不存在时间列！[('id', 'string'), ('time', 'string')]"
     ]
    }
   ],
   "source": [
    "time_col = is_time_col(df)\n",
    "time_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098b340",
   "metadata": {},
   "source": [
    "## 日期"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448f9d9",
   "metadata": {},
   "source": [
    "### 当前日期"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc89db",
   "metadata": {},
   "source": [
    "获取当前系统日期。默认情况下，数据将以yyyy-dd-mm格式返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4ac728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:48:48.046902Z",
     "start_time": "2023-11-09T09:48:33.683758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2023-11-09|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.current_date().alias(\"current_date\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527b9d9",
   "metadata": {},
   "source": [
    "### 日期格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b487e",
   "metadata": {},
   "source": [
    "解析日期并转换yyyy-dd-mm为MM-dd-yyyy格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8adfde6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:49:52.738819Z",
     "start_time": "2023-11-09T09:49:38.569633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      time|date_format|\n",
      "+----------+-----------+\n",
      "|2020-02-01| 02-01-2020|\n",
      "|2019-03-01| 03-01-2019|\n",
      "|2021-03-01| 03-01-2021|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"), F.date_format(F.col(\"time\"), \"MM-dd-yyyy\").alias(\"date_format\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e46f05",
   "metadata": {},
   "source": [
    "### 字符串转日期格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee8a817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:52:34.121823Z",
     "start_time": "2023-11-09T09:52:20.067577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      time|   to_date|\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"), F.to_date(F.col(\"time\"), \"yyyy-MM-dd HH:mm:ss\").alias(\"to_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed41ac",
   "metadata": {},
   "source": [
    "### 日期之差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d6ca3",
   "metadata": {},
   "source": [
    "可以直接两个时间戳相减。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91738968",
   "metadata": {},
   "source": [
    "### 天数之差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d100dc52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:53:40.906330Z",
     "start_time": "2023-11-09T09:53:26.495630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      time|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|    1377|\n",
      "|2019-03-01|    1714|\n",
      "|2021-03-01|     983|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"), F.datediff(F.current_date(), F.col(\"time\")).alias(\"datediff\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544a9e5",
   "metadata": {},
   "source": [
    "### 月数之差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f88f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:54:37.484476Z",
     "start_time": "2023-11-09T09:54:23.306809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|      time|months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|   45.25806452|\n",
      "|2019-03-01|   56.25806452|\n",
      "|2021-03-01|   32.25806452|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"), F.months_between(F.current_date(),F.col(\"time\")).alias(\"months_between\")  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e52a2",
   "metadata": {},
   "source": [
    "### 秒数之差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b21f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col, lit\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"TimeDifferenceCalculation\").getOrCreate()\n",
    "\n",
    "# 假设你已经有一个 DataFrame 名为 dataframe\n",
    "# 这里创建一个示例 DataFrame\n",
    "data = [\n",
    "    (\"2023-01-01 10:00:00\", \"2023-01-01 10:10:00\")\n",
    "]\n",
    "columns = [\"start_time\", \"stop_time\"]\n",
    "dataframe = spark.createDataFrame(data, columns)\n",
    "\n",
    "# 假设 delay 是一个已知的秒数\n",
    "delay = 600\n",
    "\n",
    "# 将字符串类型的时间列转换为时间戳类型\n",
    "dataframe = dataframe.withColumn(\"start_time\", to_timestamp(col(\"start_time\")))\n",
    "dataframe = dataframe.withColumn(\"stop_time\", to_timestamp(col(\"stop_time\")))\n",
    "\n",
    "# 计算时间差（以秒为单位）\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "time_diff_seconds = unix_timestamp(col(\"stop_time\")) - unix_timestamp(col(\"start_time\"))\n",
    "\n",
    "# 使用 when 函数来根据条件更新列\n",
    "from pyspark.sql.functions import when\n",
    "dataframe = dataframe.withColumn(\n",
    "    \"start_time\",\n",
    "    when(time_diff_seconds < delay, to_timestamp(lit(None))).otherwise(col(\"start_time\"))\n",
    ")\n",
    "dataframe = dataframe.withColumn(\n",
    "    \"stop_time\",\n",
    "    when(time_diff_seconds < delay, to_timestamp(lit(None))).otherwise(col(\"stop_time\"))\n",
    ")\n",
    "\n",
    "# 显示结果\n",
    "dataframe.show()\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2501365",
   "metadata": {},
   "source": [
    "### 截断指定单位的日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb50d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:57:17.745801Z",
     "start_time": "2023-11-09T09:57:03.538262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----------+\n",
      "|      time|Month_Trunc|Month_Year|Month_Trunc|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2020-02-03| 2020-02-01|2020-01-01| 2020-02-01|\n",
      "|2019-03-05| 2019-03-01|2019-01-01| 2019-03-01|\n",
      "|2021-03-09| 2021-03-01|2021-01-01| 2021-03-01|\n",
      "+----------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"),\n",
    "          F.trunc(F.col(\"time\"), \"Month\").alias(\"Month_Trunc\"),\n",
    "          F.trunc(F.col(\"time\"), \"Year\").alias(\"Month_Year\"),\n",
    "          F.trunc(F.col(\"time\"), \"Month\").alias(\"Month_Trunc\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0af3b4",
   "metadata": {},
   "source": [
    "### 月、日加减法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc6af4f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:57:38.787985Z",
     "start_time": "2023-11-09T09:57:24.394510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|      time|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-03|2020-05-03|2019-11-03|2020-02-07|2020-01-30|\n",
      "|2019-03-05|2019-06-05|2018-12-05|2019-03-09|2019-03-01|\n",
      "|2021-03-09|2021-06-09|2020-12-09|2021-03-13|2021-03-05|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"),\n",
    "          F.add_months(F.col(\"time\"), 3).alias(\"add_months\"),\n",
    "          F.add_months(F.col(\"time\"), -3).alias(\"sub_months\"),\n",
    "          F.date_add(F.col(\"time\"), 4).alias(\"date_add\"),\n",
    "          F.date_sub(F.col(\"time\"), 4).alias(\"date_sub\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7850c",
   "metadata": {},
   "source": [
    "### 年、月、下一天、一年中第几个星期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24a94d77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:58:11.518763Z",
     "start_time": "2023-11-09T09:57:57.387804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+----------+----------+\n",
      "|      time|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-03|2020|    2|2020-02-09|         6|\n",
      "|2019-03-05|2019|    3|2019-03-10|        10|\n",
      "|2021-03-09|2021|    3|2021-03-14|        10|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"),\n",
    "          F.year(F.col(\"time\")).alias(\"year\"),\n",
    "          F.month(F.col(\"time\")).alias(\"month\"),\n",
    "          F.next_day(F.col(\"time\"), \"Sunday\").alias(\"next_day\"),\n",
    "          F.weekofyear(F.col(\"time\")).alias(\"weekofyear\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65e892",
   "metadata": {},
   "source": [
    "### 星期几、月日、年日"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a7055",
   "metadata": {},
   "source": [
    "- 查询星期几\n",
    "- 一个月中的第几天\n",
    "- 一年中的第几天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f583b68b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:59:21.729499Z",
     "start_time": "2023-11-09T09:59:07.611194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|      time|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-03|        2|         3|       34|\n",
      "|2019-03-05|        3|         5|       64|\n",
      "|2021-03-09|        3|         9|       68|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    F.col(\"time\"),\n",
    "    F.dayofweek(F.col(\"time\")).alias(\"dayofweek\"),\n",
    "    F.dayofmonth(F.col(\"time\")).alias(\"dayofmonth\"),\n",
    "    F.dayofyear(F.col(\"time\")).alias(\"dayofyear\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd5c07",
   "metadata": {},
   "source": [
    "## 时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f7c160f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:00:12.894567Z",
     "start_time": "2023-11-09T09:59:58.618813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |time                   |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[\"1\", \"02-01-2020 11 01 19 06\"], [\"2\", \"03-01-2019 12 01 19 406\"],\n",
    "        [\"3\", \"03-01-2021 12 01 19 406\"]]\n",
    "df2 = spark.createDataFrame(data, [\"id\", \"time\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b18f3f",
   "metadata": {},
   "source": [
    "### 返回当前时间戳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "228de2f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:00:48.961751Z",
     "start_time": "2023-11-09T10:00:34.599830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   current_timestamp|\n",
      "+--------------------+\n",
      "|2023-11-09 18:00:...|\n",
      "|2023-11-09 18:00:...|\n",
      "|2023-11-09 18:00:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#默认格式yyyy-MM-dd HH:mm:ss\n",
    "df2.select(F.current_timestamp().alias(\"current_timestamp\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb974d",
   "metadata": {},
   "source": [
    "### 字符串转为时间戳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b99d06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:02:05.539309Z",
     "start_time": "2023-11-09T10:01:51.244951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|time                   |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\n",
    "    F.col(\"time\"),\n",
    "    F.to_timestamp(F.col(\"time\"),\"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf2167",
   "metadata": {},
   "source": [
    "### 获取小时、分钟、秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5ac1c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:03:19.900934Z",
     "start_time": "2023-11-09T10:03:05.467779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|time                   |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据\n",
    "data = [[\"1\", \"2020-02-01 11:01:19.06\"], [\"2\", \"2019-03-01 12:01:19.406\"],\n",
    "        [\"3\", \"2021-03-01 12:01:19.406\"]]\n",
    "df3 = spark.createDataFrame(data, [\"id\", \"time\"])\n",
    "\n",
    "# 提取小时、分钟、秒\n",
    "df3.select(F.col(\"time\"),\n",
    "           F.hour(F.col(\"time\")).alias(\"hour\"),\n",
    "           F.minute(F.col(\"time\")).alias(\"minute\"),\n",
    "           F.second(F.col(\"time\")).alias(\"second\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f149e4",
   "metadata": {},
   "source": [
    "## 滑动窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0722cc8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T06:08:38.841265Z",
     "start_time": "2024-11-11T06:08:38.837008Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846591d",
   "metadata": {},
   "source": [
    "Window函数分类为三种:\n",
    "- 排名函数 ranking functions包括:\n",
    "  - row_number()\n",
    "  - rank()\n",
    "  - dense_rank()\n",
    "  - percent_rank()\n",
    "  - ntile()\n",
    "  - orderBy()\n",
    "- 解析函数 analytic functions包括:\n",
    "  - cume_dist()\n",
    "  - lag()\n",
    "  - lead()\n",
    "- 聚合函数 aggregate functions包括:\n",
    "  - sum()\n",
    "  - first()\n",
    "  - last()\n",
    "  - max()\n",
    "  - min()\n",
    "  - mean()\n",
    "  - stddev()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9840e9",
   "metadata": {},
   "source": [
    "### 创建一个 PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84bd638a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T06:09:10.344481Z",
     "start_time": "2024-11-11T06:08:42.703550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|name   |department|salary|\n",
      "+-------+----------+------+\n",
      "|Ali    |Sales     |8000  |\n",
      "|Bob    |Sales     |7000  |\n",
      "|Cindy  |Sales     |7500  |\n",
      "|Davd   |Finance   |10000 |\n",
      "|Elena  |Sales     |8000  |\n",
      "|Fancy  |Finance   |12000 |\n",
      "|George |Finance   |11000 |\n",
      "|Haffman|Marketing |7000  |\n",
      "|Ilaja  |Marketing |8000  |\n",
      "|Joey   |Sales     |9000  |\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_salary = [\n",
    "    (\"Ali\", \"Sales\", 8000),\n",
    "    (\"Bob\", \"Sales\", 7000),\n",
    "    (\"Cindy\", \"Sales\", 7500),\n",
    "    (\"Davd\", \"Finance\", 10000),\n",
    "    (\"Elena\", \"Sales\", 8000),\n",
    "    (\"Fancy\", \"Finance\", 12000),\n",
    "    (\"George\", \"Finance\", 11000),\n",
    "    (\"Haffman\", \"Marketing\", 7000),\n",
    "    (\"Ilaja\", \"Marketing\", 8000),\n",
    "    (\"Joey\", \"Sales\", 9000)]\n",
    " \n",
    "columns= [\"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = employee_salary, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee83e98",
   "metadata": {},
   "source": [
    "### 窗口函数 ranking functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9a6ac",
   "metadata": {},
   "source": [
    "#### row_number()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3370f50",
   "metadata": {},
   "source": [
    "row_number() 窗口函数用于给出从1开始到每个窗口分区的结果的连续行号。 与 groupBy 不同 Window 以 partitionBy 作为分组条件，orderBy 对 Window 分组内的数据进行排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1db9f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:07:02.969539Z",
     "start_time": "2023-11-24T10:06:46.496623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----------+\n",
      "|   name|department|salary|row_number|\n",
      "+-------+----------+------+----------+\n",
      "|   Davd|   Finance| 10000|         1|\n",
      "| George|   Finance| 11000|         2|\n",
      "|  Fancy|   Finance| 12000|         3|\n",
      "|Haffman| Marketing|  7000|         1|\n",
      "|  Ilaja| Marketing|  8000|         2|\n",
      "|    Bob|     Sales|  7000|         1|\n",
      "|  Cindy|     Sales|  7500|         2|\n",
      "|    Ali|     Sales|  8000|         3|\n",
      "|  Elena|     Sales|  8000|         4|\n",
      "|   Joey|     Sales|  9000|         5|\n",
      "+-------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 以 department 字段进行分组，以 salary 倒序排序\n",
    "# 按照部门对薪水排名，薪水最低的为第一名\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.asc(\"salary\"))\n",
    "# 分组内增加 row_number\n",
    "df_part = df.withColumn(\n",
    "    \"row_number\", \n",
    "    F.row_number().over(windowSpec)\n",
    ")\n",
    "df_part.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88ca2e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:07:19.468051Z",
     "start_time": "2023-11-24T10:07:02.972042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+----------+\n",
      "|  name|department|salary|row_number|\n",
      "+------+----------+------+----------+\n",
      "|George|   Finance| 11000|         2|\n",
      "| Ilaja| Marketing|  8000|         2|\n",
      "| Cindy|     Sales|  7500|         2|\n",
      "+------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_part.where(F.col('row_number') == 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6309b2",
   "metadata": {},
   "source": [
    "#### rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4f7b2",
   "metadata": {},
   "source": [
    "rank()用来给按照指定列排序的分组窗增加一个排序的序号，\n",
    "\n",
    "如果有相同数值，则排序数相同，下一个序数顺延一位。来看如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec0bd1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:07:36.401426Z",
     "start_time": "2023-11-24T10:07:19.469797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n",
      "|   name|department|salary|rank|\n",
      "+-------+----------+------+----+\n",
      "|  Fancy|   Finance| 12000|   1|\n",
      "| George|   Finance| 11000|   2|\n",
      "|   Davd|   Finance| 10000|   3|\n",
      "|  Ilaja| Marketing|  8000|   1|\n",
      "|Haffman| Marketing|  7000|   2|\n",
      "|   Joey|     Sales|  9000|   1|\n",
      "|    Ali|     Sales|  8000|   2|\n",
      "|  Elena|     Sales|  8000|   2|\n",
      "|  Cindy|     Sales|  7500|   4|\n",
      "|    Bob|     Sales|  7000|   5|\n",
      "+-------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 rank 排序，都是8000的薪水，就同列第二\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df_rank = df.withColumn(\"rank\", F.rank().over(windowSpec))\n",
    "df_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5b4e504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:07:53.314386Z",
     "start_time": "2023-11-24T10:07:36.405177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n",
      "|   name|department|salary|rank|\n",
      "+-------+----------+------+----+\n",
      "| George|   Finance| 11000|   2|\n",
      "|Haffman| Marketing|  7000|   2|\n",
      "|    Ali|     Sales|  8000|   2|\n",
      "|  Elena|     Sales|  8000|   2|\n",
      "+-------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rank.where(F.col(\"rank\")==\"2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0fe188",
   "metadata": {},
   "source": [
    "#### dense_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8fd6fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:08:09.937061Z",
     "start_time": "2023-11-24T10:07:53.314386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----------+\n",
      "|   name|department|salary|dense_rank|\n",
      "+-------+----------+------+----------+\n",
      "|  Fancy|   Finance| 12000|         1|\n",
      "| George|   Finance| 11000|         2|\n",
      "|   Davd|   Finance| 10000|         3|\n",
      "|  Ilaja| Marketing|  8000|         1|\n",
      "|Haffman| Marketing|  7000|         2|\n",
      "|   Joey|     Sales|  9000|         1|\n",
      "|    Ali|     Sales|  8000|         2|\n",
      "|  Elena|     Sales|  8000|         2|\n",
      "|  Cindy|     Sales|  7500|         3|\n",
      "|    Bob|     Sales|  7000|         4|\n",
      "+-------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 注意 rank 排序，8000虽然为同列第二，但7500属于第4名\n",
    "# dense_rank()中， 8000同列第二后，7500属于第3名\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"dense_rank\", F.dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707218a",
   "metadata": {},
   "source": [
    "#### percent_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da269f2",
   "metadata": {},
   "source": [
    "计算不同数值的百分比排序数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c1962c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:08:26.751393Z",
     "start_time": "2023-11-24T10:08:09.938822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------+\n",
      "|   name|department|salary|percent_rank|\n",
      "+-------+----------+------+------------+\n",
      "|  Fancy|   Finance| 12000|         0.0|\n",
      "| George|   Finance| 11000|         0.5|\n",
      "|   Davd|   Finance| 10000|         1.0|\n",
      "|  Ilaja| Marketing|  8000|         0.0|\n",
      "|Haffman| Marketing|  7000|         1.0|\n",
      "|   Joey|     Sales|  9000|         0.0|\n",
      "|    Ali|     Sales|  8000|        0.25|\n",
      "|  Elena|     Sales|  8000|        0.25|\n",
      "|  Cindy|     Sales|  7500|        0.75|\n",
      "|    Bob|     Sales|  7000|         1.0|\n",
      "+-------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"percent_rank\",F.percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336fbfa",
   "metadata": {},
   "source": [
    "上述结果可以理解为将 dense_rank() 的结果进行归一化， 即可得到0-1以内的百分数。percent_rank() 与 SQL 中的 PERCENT_RANK 函数效果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beed718",
   "metadata": {},
   "source": [
    "#### ntile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c3f65",
   "metadata": {},
   "source": [
    "ntile()可将分组的数据按照指定数值n切分为n个部分， 每一部分按照行的先后给定相同的序数。例如n指定为2，则将组内数据分为两个部分， 第一部分序号为1，第二部分序号为2。理论上两部分数据行数是均等的， 但当数据为奇数行时，中间的那一行归到前一部分。\n",
    "\n",
    "按照部门对数据进行分组，然后在组内按照薪水高低进行排序， 再使用 ntile() 将组内数据切分为两个部分。结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70bd6fe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:08:55.263499Z",
     "start_time": "2023-11-24T10:08:38.247734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+\n",
      "|   name|department|salary|ntile|\n",
      "+-------+----------+------+-----+\n",
      "|  Fancy|   Finance| 12000|    1|\n",
      "| George|   Finance| 11000|    1|\n",
      "|   Davd|   Finance| 10000|    2|\n",
      "|  Ilaja| Marketing|  8000|    1|\n",
      "|Haffman| Marketing|  7000|    2|\n",
      "|   Joey|     Sales|  9000|    1|\n",
      "|    Ali|     Sales|  8000|    1|\n",
      "|  Elena|     Sales|  8000|    1|\n",
      "|  Cindy|     Sales|  7500|    2|\n",
      "|    Bob|     Sales|  7000|    2|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 按照部门对数据进行分组，然后在组内按照薪水高低进行排序 \n",
    "windowSpec = Window.partitionBy(\n",
    "    \"department\").orderBy(F.desc(\"salary\"))\n",
    "# 使用ntile() 将组内数据切分为两个部分\n",
    "df.withColumn(\"ntile\", F.ntile(2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b164342b",
   "metadata": {},
   "source": [
    "###  分析函数 Analytic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edcf0d5",
   "metadata": {},
   "source": [
    "#### cume_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06de27",
   "metadata": {},
   "source": [
    "cume_dist()函数用来获取数值的累进分布值，看如下例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "532e084c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:09:54.593116Z",
     "start_time": "2023-11-24T10:09:37.601198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------------+\n",
      "|   name|department|salary|         cume_dist|\n",
      "+-------+----------+------+------------------+\n",
      "|  Fancy|   Finance| 12000|0.3333333333333333|\n",
      "| George|   Finance| 11000|0.6666666666666666|\n",
      "|   Davd|   Finance| 10000|               1.0|\n",
      "|  Ilaja| Marketing|  8000|               0.5|\n",
      "|Haffman| Marketing|  7000|               1.0|\n",
      "|   Joey|     Sales|  9000|               0.2|\n",
      "|    Ali|     Sales|  8000|               0.6|\n",
      "|  Elena|     Sales|  8000|               0.6|\n",
      "|  Cindy|     Sales|  7500|               0.8|\n",
      "|    Bob|     Sales|  7000|               1.0|\n",
      "+-------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"cume_dist\", F.cume_dist().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deefa82d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:10:11.383010Z",
     "start_time": "2023-11-24T10:09:54.593116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------+\n",
      "|   name|department|salary|percent_rank|\n",
      "+-------+----------+------+------------+\n",
      "|  Fancy|   Finance| 12000|         0.0|\n",
      "| George|   Finance| 11000|         0.5|\n",
      "|   Davd|   Finance| 10000|         1.0|\n",
      "|  Ilaja| Marketing|  8000|         0.0|\n",
      "|Haffman| Marketing|  7000|         1.0|\n",
      "|   Joey|     Sales|  9000|         0.0|\n",
      "|    Ali|     Sales|  8000|        0.25|\n",
      "|  Elena|     Sales|  8000|        0.25|\n",
      "|  Cindy|     Sales|  7500|        0.75|\n",
      "|    Bob|     Sales|  7000|         1.0|\n",
      "+-------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 和 percent_rank 对比一下\n",
    "df.withColumn(\n",
    "    'percent_rank',\n",
    "    F.percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaacff01",
   "metadata": {},
   "source": [
    "#### lag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc92009",
   "metadata": {},
   "source": [
    "lag() 函数用于寻找按照指定列排好序的分组内每个数值的上一个数值，\n",
    "\n",
    "通俗的说，就是数值排好序以后，寻找排在每个数值的上一个数值。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52e0e92f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:10:44.621869Z",
     "start_time": "2023-11-24T10:10:28.051599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+\n",
      "|   name|department|salary|  lag|\n",
      "+-------+----------+------+-----+\n",
      "|  Fancy|   Finance| 12000| null|\n",
      "| George|   Finance| 11000|12000|\n",
      "|   Davd|   Finance| 10000|11000|\n",
      "|  Ilaja| Marketing|  8000| null|\n",
      "|Haffman| Marketing|  7000| 8000|\n",
      "|   Joey|     Sales|  9000| null|\n",
      "|    Ali|     Sales|  8000| 9000|\n",
      "|  Elena|     Sales|  8000| 8000|\n",
      "|  Cindy|     Sales|  7500| 8000|\n",
      "|    Bob|     Sales|  7000| 7500|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 相当于滞后项\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"lag\", F.lag(\"salary\",1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f70ae",
   "metadata": {},
   "source": [
    "#### lead()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a0b62",
   "metadata": {},
   "source": [
    "lead() 用于获取排序后的数值的下一个，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa87ac80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:11:28.859972Z",
     "start_time": "2023-11-24T10:11:12.096982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+\n",
      "|   name|department|salary| lead|\n",
      "+-------+----------+------+-----+\n",
      "|  Fancy|   Finance| 12000|11000|\n",
      "| George|   Finance| 11000|10000|\n",
      "|   Davd|   Finance| 10000| null|\n",
      "|  Ilaja| Marketing|  8000| 7000|\n",
      "|Haffman| Marketing|  7000| null|\n",
      "|   Joey|     Sales|  9000| 8000|\n",
      "|    Ali|     Sales|  8000| 8000|\n",
      "|  Elena|     Sales|  8000| 7500|\n",
      "|  Cindy|     Sales|  7500| 7000|\n",
      "|    Bob|     Sales|  7000| null|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 和滞后项相反，提前一位\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"lead\",F.lead(\"salary\",1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7f338",
   "metadata": {},
   "source": [
    "### 聚合函数 Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809407d",
   "metadata": {},
   "source": [
    "常见的聚合函数有avg, sum, min, max, count, approx_count_distinct()等，我们用如下代码来同时使用这些函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2e97cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:12:12.091181Z",
     "start_time": "2023-11-24T10:11:55.153847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\pyspark\\sql\\functions.py:2610: FutureWarning: Deprecated in 2.1, use approx_count_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 2.1, use approx_count_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+-------+-----+-----+-----+-----+--------------+\n",
      "|   name|department|salary|row|    avg|  sum|  min|  max|count|distinct_count|\n",
      "+-------+----------+------+---+-------+-----+-----+-----+-----+--------------+\n",
      "|  Fancy|   Finance| 12000|  1|11000.0|33000|10000|12000|    3|             3|\n",
      "| George|   Finance| 11000|  2|11000.0|33000|10000|12000|    3|             3|\n",
      "|   Davd|   Finance| 10000|  3|11000.0|33000|10000|12000|    3|             3|\n",
      "|  Ilaja| Marketing|  8000|  1| 7500.0|15000| 7000| 8000|    2|             2|\n",
      "|Haffman| Marketing|  7000|  2| 7500.0|15000| 7000| 8000|    2|             2|\n",
      "|   Joey|     Sales|  9000|  1| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "|    Ali|     Sales|  8000|  2| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "|  Elena|     Sales|  8000|  3| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "|  Cindy|     Sales|  7500|  4| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "|    Bob|     Sales|  7000|  5| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "+-------+----------+------+---+-------+-----+-----+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分组，并对组内数据排序\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "# 仅分组\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "df.withColumn(\"row\", F.row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", F.avg(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", F.sum(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", F.min(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", F.max(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"count\", F.count(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"distinct_count\", F.approxCountDistinct(\"salary\").over(windowSpecAgg)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72138468",
   "metadata": {},
   "source": [
    "需要注意的是 approx_count_distinct() 函数适用与窗函数的统计， 而在groupby中通常用countDistinct()来代替该函数，用来求组内不重复的数值的条数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c031d",
   "metadata": {},
   "source": [
    "从结果来看，统计值基本上是按照部门分组，统计组内的salary情况。 如果我们只想要保留部门的统计结果，而将每个人的实际情况去掉，可以采用如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04fe5b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:12:55.809700Z",
     "start_time": "2023-11-24T10:12:39.010112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+-----+-----+-----+--------------+\n",
      "|department|    avg|  sum|  min|  max|count|distinct_count|\n",
      "+----------+-------+-----+-----+-----+-----+--------------+\n",
      "|   Finance|11000.0|33000|10000|12000|    3|             3|\n",
      "| Marketing| 7500.0|15000| 7000| 8000|    2|             2|\n",
      "|     Sales| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "+----------+-------+-----+-----+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "df = df.withColumn(\"row\", F.row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", F.avg(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", F.sum(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", F.min(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", F.max(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"count\", F.count(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"distinct_count\", F.approx_count_distinct(\"salary\").over(windowSpecAgg))\n",
    "\n",
    "# 仅选取分组第一行数据\n",
    "# 用F.col 去选row 行，怪怪的\n",
    "df_part  = df.where(F.col(\"row\")==1)\n",
    "df_part.select(\"department\",\"avg\",\"sum\",\"min\",\"max\",\"count\",\"distinct_count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeaff00",
   "metadata": {},
   "source": [
    "## 常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5479c1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:43:59.835610Z",
     "start_time": "2024-11-19T01:43:59.807670Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载和准备时间序列数据\n",
    "data = [(\"2022-01-01\", 20), (\"2022-01-02\", 10), (\"2022-01-03\", 10),\n",
    "        (\"2022-01-04\", 10), (\"2022-01-05\", 10), (\"2022-01-06\", 10),\n",
    "        (\"2022-01-07\", 40), (\"2022-01-08\", 10), (\"2022-01-09\", 10), (\"2022-01-10\", 10),\n",
    "        (\"2022-01-11\", 10), (\"2022-01-12\", 10), (\"2022-01-13\", 10),\n",
    "        (\"2022-01-14\", 40)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"timestamp\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e974aa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T09:08:11.004643Z",
     "start_time": "2024-11-18T09:07:51.396362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------+\n",
      "| timestamp|value|moving_average|\n",
      "+----------+-----+--------------+\n",
      "|2022-01-01|   10|          10.0|\n",
      "|2022-01-02|   15|          12.5|\n",
      "|2022-01-03|  -25|           0.0|\n",
      "|2022-01-04|  -25|         -6.25|\n",
      "|2022-01-05|  -30|         -11.0|\n",
      "|2022-01-06|   10|         -11.0|\n",
      "|2022-01-07|   40|          -6.0|\n",
      "+----------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "# 定义滑动窗口规范\n",
    "windowSpec = Window.orderBy(F.col(\"timestamp\")).rowsBetween(-4, 0)\n",
    "\n",
    "# 应用窗口规范和转换操作\n",
    "df1 = df.withColumn(\"moving_average\", F.avg(F.col(\"value\")).over(windowSpec))\n",
    "\n",
    "# 显示结果\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8208704b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T08:57:44.486628Z",
     "start_time": "2024-07-11T08:57:27.026752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|value_bu|\n",
      "+--------+\n",
      "|      10|\n",
      "|      15|\n",
      "|      25|\n",
      "|      25|\n",
      "|      30|\n",
      "|      10|\n",
      "|      40|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 加载和准备时间序列数据\n",
    "data = [(\"2022-01-01\", -10), (\"2022-01-02\", -15), (\"2022-01-03\", -25),\n",
    "        (\"2022-01-04\", -25), (\"2022-01-05\", -30), (\"2022-01-06\", -10),\n",
    "        (\"2022-01-07\", -40)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"timestamp\", \"value\"])\n",
    "\n",
    "# 定义滑动窗口规范\n",
    "windowSpec = Window.orderBy(col(\"timestamp\")).rowsBetween(-2, 2)\n",
    "\n",
    "# 判定正负\n",
    "windowSpec = Window.orderBy(col(\"timestamp\")).rowsBetween(-2, 2)\n",
    "sum_ = F.sum(F.col(\"value\")).over(windowSpec)\n",
    "sum_sum = F.abs(sum_)\n",
    "sum_abs = F.sum(F.abs(F.col(\"value\"))).over(windowSpec)\n",
    "bu_column = F.when((sum_sum-sum_abs) == 0, F.abs(F.col(\"value\"))).otherwise(F.col(\"value\")).alias(f\"value_bu\")\n",
    "\n",
    "# 显示结果\n",
    "df.select(bu_column).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc8b35b-35b3-4524-a066-bbe682c8aad2",
   "metadata": {},
   "source": [
    "# 缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f04f70a-94ac-450c-bbdb-07f4ab6dec94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T11:59:09.578283Z",
     "iopub.status.busy": "2024-06-13T11:59:09.576291Z",
     "iopub.status.idle": "2024-06-13T11:59:09.601084Z",
     "shell.execute_reply": "2024-06-13T11:59:09.599082Z",
     "shell.execute_reply.started": "2024-06-13T11:59:09.577284Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88d8ad9-238f-4869-b9e2-5b10868a0e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T11:59:54.189034Z",
     "iopub.status.busy": "2024-06-13T11:59:54.187992Z",
     "iopub.status.idle": "2024-06-13T12:00:01.295970Z",
     "shell.execute_reply": "2024-06-13T12:00:01.294971Z",
     "shell.execute_reply.started": "2024-06-13T11:59:54.189034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|income|\n",
      "+---+------+\n",
      "|1.0|   NaN|\n",
      "|2.0|   NaN|\n",
      "|2.0|   NaN|\n",
      "|NaN|   3.0|\n",
      "|4.0|   4.0|\n",
      "|5.0|   5.0|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1.0, float(\"nan\")), (2.0, float(\"nan\")),\n",
    "                            (2.0, float(\"nan\")), (float(\"nan\"), 3.0),\n",
    "                            (4.0, 4.0), (5.0, 5.0)], [\"age\", \"income\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357451e4-2584-4a70-8790-ad6f1a84833a",
   "metadata": {},
   "source": [
    "## 删除缺失值的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d5364ba-fc5c-4abb-8774-48f0cf2d5d6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:00:21.032358Z",
     "iopub.status.busy": "2024-06-13T12:00:21.031371Z",
     "iopub.status.idle": "2024-06-13T12:00:27.972853Z",
     "shell.execute_reply": "2024-06-13T12:00:27.971254Z",
     "shell.execute_reply.started": "2024-06-13T12:00:21.032358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|income|\n",
      "+---+------+\n",
      "|4.0|   4.0|\n",
      "|5.0|   5.0|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 仅保留不包含缺失值的行\n",
    "df1 = df.dropna()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce82c1d-9eb6-42c1-ab9a-1c486c12884b",
   "metadata": {},
   "source": [
    "## 使用统计值填充缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb61995-c9f2-4bb4-b0d2-cfd5bd406aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:00:42.791482Z",
     "iopub.status.busy": "2024-06-13T12:00:42.790487Z",
     "iopub.status.idle": "2024-06-13T12:00:57.056749Z",
     "shell.execute_reply": "2024-06-13T12:00:57.055232Z",
     "shell.execute_reply.started": "2024-06-13T12:00:42.791482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|income|\n",
      "+---+------+\n",
      "|1.0|   4.0|\n",
      "|2.0|   4.0|\n",
      "|2.0|   4.0|\n",
      "|2.8|   3.0|\n",
      "|4.0|   4.0|\n",
      "|5.0|   5.0|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用Imputer将缺失值进行插值，均值\n",
    "imputer = Imputer(inputCols=[\"age\", \"income\"],\n",
    "                  outputCols=[\"age\", \"income\"]).setStrategy(\"mean\")\n",
    "\n",
    "df2 = imputer.fit(df).transform(df)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e732afd-1dd8-41a5-bbd7-75ea85b64f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:00:59.235791Z",
     "iopub.status.busy": "2024-06-13T12:00:59.232778Z",
     "iopub.status.idle": "2024-06-13T12:01:13.381017Z",
     "shell.execute_reply": "2024-06-13T12:01:13.379938Z",
     "shell.execute_reply.started": "2024-06-13T12:00:59.234785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|income|\n",
      "+---+------+\n",
      "|1.0|   4.0|\n",
      "|2.0|   4.0|\n",
      "|2.0|   4.0|\n",
      "|2.0|   3.0|\n",
      "|4.0|   4.0|\n",
      "|5.0|   5.0|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用Imputer将缺失值进行插值，中位数\n",
    "imputer = Imputer(inputCols=[\"age\", \"income\"],\n",
    "                  outputCols=[\"age\", \"income\"]).setStrategy(\"median\")\n",
    "\n",
    "df4 = imputer.fit(df).transform(df)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7921df1-7b6d-4b02-8f5b-316ead8f752a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:01:13.384362Z",
     "iopub.status.busy": "2024-06-13T12:01:13.383899Z",
     "iopub.status.idle": "2024-06-13T12:01:27.743074Z",
     "shell.execute_reply": "2024-06-13T12:01:27.741542Z",
     "shell.execute_reply.started": "2024-06-13T12:01:13.384362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|income|\n",
      "+---+------+\n",
      "|1.0|   3.0|\n",
      "|2.0|   3.0|\n",
      "|2.0|   3.0|\n",
      "|2.0|   3.0|\n",
      "|4.0|   4.0|\n",
      "|5.0|   5.0|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用Imputer将缺失值进行插值，众数\n",
    "imputer = Imputer(inputCols=[\"age\", \"income\"],\n",
    "                  outputCols=[\"age\", \"income\"]).setStrategy(\"mode\")\n",
    "\n",
    "df5 = imputer.fit(df).transform(df)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4de09-7ad4-4008-bd69-84ddccb1af76",
   "metadata": {},
   "source": [
    "## 使用任意值填充缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9d1bda1-f82f-48c5-b34c-d50eaefc50a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:01:29.447889Z",
     "iopub.status.busy": "2024-06-13T12:01:29.446209Z",
     "iopub.status.idle": "2024-06-13T12:01:36.541389Z",
     "shell.execute_reply": "2024-06-13T12:01:36.539388Z",
     "shell.execute_reply.started": "2024-06-13T12:01:29.447889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|income|\n",
      "+---+------+\n",
      "|1.0|   0.0|\n",
      "|2.0|   0.0|\n",
      "|2.0|   0.0|\n",
      "|0.0|   3.0|\n",
      "|4.0|   4.0|\n",
      "|5.0|   5.0|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 填充空白值为0\n",
    "df3 = df.fillna(0)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2beb7-8614-465f-a765-aab4e6b0f5df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "212.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
